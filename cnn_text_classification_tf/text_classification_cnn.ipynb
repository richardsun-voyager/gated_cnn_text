{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment aims to classifiy 20newsgroup through convolutional neural network models. Based on the original [paper](http://arxiv.org/abs/1408.5882), [dennybritz](https://github.com/dennybritz/cnn-text-classification-tf) realized this model on sentimental classification using tensorflow. And he was generous to share his work at [Github](https://github.com/dennybritz/cnn-text-classification-tf). Refering to his work,  I made some modifications on the model and classify 20newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Texts\n",
    "I have preprocess the original texts and saved them as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "test_data = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in train_data.text])\n",
    "#Cut long articles to 800 words. Pad short ones\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(800)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(list(vocab_processor.transform(test_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_data.target, test_data.target\n",
    "y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "y_test = np.array(y_test).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encode the label as one-hot code\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y_train = ohe.fit_transform(y_train)\n",
    "y_test = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Restore the values from sparse matrix\n",
    "y_train = np.array([item.toarray().reshape(-1) for item in y_train])\n",
    "y_test = np.array([item.toarray().reshape(-1) for item in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=64,\n",
    "            filter_sizes= [2, 3, 4, 5, 6],\n",
    "            num_filters=32,\n",
    "            l2_reg_lambda=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #train_op = optimizer.minimize(cnn.loss)\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "    \n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    for g, v in grads_and_vars:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26T17:40:57.353418: step 100, loss 5.05575, acc 0.09375\n",
      "2017-11-26T17:41:23.619794: step 200, loss 4.30428, acc 0.03125\n",
      "2017-11-26T17:41:49.229355: step 300, loss 3.83423, acc 0.03125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-300\n",
      "\n",
      "2017-11-26T17:42:16.329830: step 400, loss 3.48603, acc 0.09375\n",
      "2017-11-26T17:42:41.700493: step 500, loss 3.10431, acc 0.125\n",
      "2017-11-26T17:43:07.069941: step 600, loss 3.27982, acc 0.125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-600\n",
      "\n",
      "2017-11-26T17:43:33.886301: step 700, loss 3.01366, acc 0.09375\n",
      "2017-11-26T17:43:59.148531: step 800, loss 2.85542, acc 0.1875\n",
      "2017-11-26T17:44:24.491942: step 900, loss 2.5636, acc 0.1875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-900\n",
      "\n",
      "2017-11-26T17:44:51.319685: step 1000, loss 2.61192, acc 0.3125\n",
      "2017-11-26T17:45:16.684476: step 1100, loss 2.483, acc 0.21875\n",
      "2017-11-26T17:45:42.103433: step 1200, loss 2.20687, acc 0.40625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-26T17:46:09.039270: step 1300, loss 1.99516, acc 0.4375\n",
      "2017-11-26T17:46:34.471284: step 1400, loss 2.39161, acc 0.34375\n",
      "2017-11-26T17:46:59.921544: step 1500, loss 1.55101, acc 0.625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-26T17:47:26.874937: step 1600, loss 1.79239, acc 0.4375\n",
      "2017-11-26T17:47:52.273831: step 1700, loss 1.61027, acc 0.5\n",
      "2017-11-26T17:48:17.625759: step 1800, loss 1.93937, acc 0.46875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-26T17:48:44.457230: step 1900, loss 1.85929, acc 0.46875\n",
      "2017-11-26T17:49:09.866246: step 2000, loss 1.53891, acc 0.65625\n",
      "2017-11-26T17:49:35.164978: step 2100, loss 1.67473, acc 0.5\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-26T17:50:01.906399: step 2200, loss 1.74854, acc 0.59375\n",
      "2017-11-26T17:50:27.289090: step 2300, loss 1.22448, acc 0.6875\n",
      "2017-11-26T17:50:52.681682: step 2400, loss 1.26081, acc 0.71875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-26T17:51:19.556402: step 2500, loss 1.05301, acc 0.875\n",
      "2017-11-26T17:51:45.008658: step 2600, loss 1.10453, acc 0.8125\n",
      "2017-11-26T17:52:10.442514: step 2700, loss 1.22966, acc 0.71875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-26T17:52:37.434210: step 2800, loss 1.01255, acc 0.84375\n",
      "2017-11-26T17:53:02.911460: step 2900, loss 0.772422, acc 0.875\n",
      "2017-11-26T17:53:28.396597: step 3000, loss 1.1295, acc 0.8125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-26T17:53:55.366053: step 3100, loss 1.19004, acc 0.71875\n",
      "2017-11-26T17:54:20.824996: step 3200, loss 0.740216, acc 0.875\n",
      "2017-11-26T17:54:46.343237: step 3300, loss 0.678148, acc 0.875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-26T17:55:13.288334: step 3400, loss 0.616518, acc 0.90625\n",
      "2017-11-26T17:55:38.808578: step 3500, loss 0.775258, acc 0.71875\n",
      "2017-11-26T17:56:04.385320: step 3600, loss 0.688251, acc 0.875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-3600\n",
      "\n",
      "2017-11-26T17:56:31.414870: step 3700, loss 0.59617, acc 0.84375\n",
      "2017-11-26T17:56:57.074816: step 3800, loss 1.19896, acc 0.75\n",
      "2017-11-26T17:57:22.684725: step 3900, loss 0.784553, acc 0.8125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-3900\n",
      "\n",
      "2017-11-26T17:57:49.705095: step 4000, loss 0.683571, acc 0.875\n",
      "2017-11-26T17:58:15.189521: step 4100, loss 0.754487, acc 0.84375\n",
      "2017-11-26T17:58:40.656342: step 4200, loss 0.69208, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-4200\n",
      "\n",
      "2017-11-26T17:59:07.684998: step 4300, loss 0.697884, acc 0.8125\n",
      "2017-11-26T17:59:33.249582: step 4400, loss 0.538916, acc 0.90625\n",
      "2017-11-26T17:59:58.753695: step 4500, loss 0.793853, acc 0.75\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-4500\n",
      "\n",
      "2017-11-26T18:00:25.782113: step 4600, loss 0.405166, acc 0.96875\n",
      "2017-11-26T18:00:51.335259: step 4700, loss 0.55946, acc 0.875\n",
      "2017-11-26T18:01:16.969861: step 4800, loss 0.502087, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-4800\n",
      "\n",
      "2017-11-26T18:01:43.993254: step 4900, loss 0.443122, acc 0.96875\n",
      "2017-11-26T18:02:09.545139: step 5000, loss 0.414286, acc 0.96875\n",
      "2017-11-26T18:02:35.104172: step 5100, loss 0.440492, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-5100\n",
      "\n",
      "2017-11-26T18:03:02.037261: step 5200, loss 0.445946, acc 0.96875\n",
      "2017-11-26T18:03:27.641187: step 5300, loss 0.462167, acc 0.9375\n",
      "2017-11-26T18:03:53.149604: step 5400, loss 0.282106, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-5400\n",
      "\n",
      "2017-11-26T18:04:20.289374: step 5500, loss 0.44028, acc 0.9375\n",
      "2017-11-26T18:04:45.813029: step 5600, loss 0.350992, acc 0.96875\n",
      "2017-11-26T18:05:11.401119: step 5700, loss 0.27074, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-5700\n",
      "\n",
      "2017-11-26T18:05:38.601462: step 5800, loss 0.316957, acc 0.96875\n",
      "2017-11-26T18:06:04.132070: step 5900, loss 0.436772, acc 0.90625\n",
      "2017-11-26T18:06:29.744070: step 6000, loss 0.403458, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-6000\n",
      "\n",
      "2017-11-26T18:06:56.871847: step 6100, loss 0.32641, acc 1\n",
      "2017-11-26T18:07:22.493200: step 6200, loss 0.403868, acc 0.9375\n",
      "2017-11-26T18:07:48.428905: step 6300, loss 0.368395, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-6300\n",
      "\n",
      "2017-11-26T18:08:15.481296: step 6400, loss 0.441873, acc 0.875\n",
      "2017-11-26T18:08:40.973954: step 6500, loss 0.20821, acc 1\n",
      "2017-11-26T18:09:06.603311: step 6600, loss 0.30912, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-6600\n",
      "\n",
      "2017-11-26T18:09:33.673985: step 6700, loss 0.459089, acc 0.9375\n",
      "2017-11-26T18:09:59.190727: step 6800, loss 0.215217, acc 1\n",
      "2017-11-26T18:10:24.778296: step 6900, loss 0.240422, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-6900\n",
      "\n",
      "2017-11-26T18:10:51.888263: step 7000, loss 0.387204, acc 0.96875\n",
      "2017-11-26T18:11:17.476160: step 7100, loss 0.227651, acc 0.96875\n",
      "2017-11-26T18:11:43.098094: step 7200, loss 0.208353, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-7200\n",
      "\n",
      "2017-11-26T18:12:10.308401: step 7300, loss 0.304821, acc 0.9375\n",
      "2017-11-26T18:12:35.943088: step 7400, loss 0.262395, acc 0.96875\n",
      "2017-11-26T18:13:01.558097: step 7500, loss 0.214186, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-7500\n",
      "\n",
      "2017-11-26T18:13:28.735713: step 7600, loss 0.182875, acc 1\n",
      "2017-11-26T18:13:54.389288: step 7700, loss 0.156148, acc 1\n",
      "2017-11-26T18:14:19.993819: step 7800, loss 0.209738, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-7800\n",
      "\n",
      "2017-11-26T18:14:47.018780: step 7900, loss 0.245356, acc 0.96875\n",
      "2017-11-26T18:15:12.731039: step 8000, loss 0.203241, acc 0.96875\n",
      "2017-11-26T18:15:38.354705: step 8100, loss 0.186845, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-8100\n",
      "\n",
      "2017-11-26T18:16:05.355112: step 8200, loss 0.190163, acc 1\n",
      "2017-11-26T18:16:30.904216: step 8300, loss 0.242392, acc 0.96875\n",
      "2017-11-26T18:16:56.503593: step 8400, loss 0.199432, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-8400\n",
      "\n",
      "2017-11-26T18:17:23.636583: step 8500, loss 0.196579, acc 0.96875\n",
      "2017-11-26T18:17:49.182693: step 8600, loss 0.22846, acc 1\n",
      "2017-11-26T18:18:14.824864: step 8700, loss 0.164078, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-8700\n",
      "\n",
      "2017-11-26T18:18:41.911535: step 8800, loss 0.463922, acc 0.9375\n",
      "2017-11-26T18:19:07.562593: step 8900, loss 0.146092, acc 1\n",
      "2017-11-26T18:19:33.145487: step 9000, loss 0.132777, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-9000\n",
      "\n",
      "2017-11-26T18:20:00.236031: step 9100, loss 0.129649, acc 1\n",
      "2017-11-26T18:20:25.880254: step 9200, loss 0.138831, acc 1\n",
      "2017-11-26T18:20:51.436918: step 9300, loss 0.213832, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-9300\n",
      "\n",
      "2017-11-26T18:21:18.457156: step 9400, loss 0.168204, acc 1\n",
      "2017-11-26T18:21:44.189003: step 9500, loss 0.198843, acc 0.96875\n",
      "2017-11-26T18:22:09.786826: step 9600, loss 0.163751, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-9600\n",
      "\n",
      "2017-11-26T18:22:36.896853: step 9700, loss 0.158423, acc 0.96875\n",
      "2017-11-26T18:23:02.463223: step 9800, loss 0.147648, acc 1\n",
      "2017-11-26T18:23:28.128608: step 9900, loss 0.139286, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-9900\n",
      "\n",
      "2017-11-26T18:23:55.181131: step 10000, loss 0.223511, acc 0.96875\n",
      "2017-11-26T18:24:20.845533: step 10100, loss 0.117123, acc 1\n",
      "2017-11-26T18:24:46.447083: step 10200, loss 0.169239, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-10200\n",
      "\n",
      "2017-11-26T18:25:13.670126: step 10300, loss 0.131972, acc 1\n",
      "2017-11-26T18:25:39.276637: step 10400, loss 0.198974, acc 0.9375\n",
      "2017-11-26T18:26:04.910919: step 10500, loss 0.122462, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-10500\n",
      "\n",
      "2017-11-26T18:26:32.021754: step 10600, loss 0.171668, acc 0.96875\n",
      "2017-11-26T18:26:57.648906: step 10700, loss 0.135332, acc 1\n",
      "2017-11-26T18:27:23.342187: step 10800, loss 0.115847, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-10800\n",
      "\n",
      "2017-11-26T18:27:50.539425: step 10900, loss 0.109543, acc 1\n",
      "2017-11-26T18:28:16.171404: step 11000, loss 0.130263, acc 1\n",
      "2017-11-26T18:28:41.813771: step 11100, loss 0.183124, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-11100\n",
      "\n",
      "2017-11-26T18:29:09.027666: step 11200, loss 0.171398, acc 1\n",
      "2017-11-26T18:29:34.776762: step 11300, loss 0.116005, acc 1\n",
      "2017-11-26T18:30:00.476818: step 11400, loss 0.0989341, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-11400\n",
      "\n",
      "2017-11-26T18:30:27.591876: step 11500, loss 0.09591, acc 1\n",
      "2017-11-26T18:30:53.277219: step 11600, loss 0.0918002, acc 1\n",
      "2017-11-26T18:31:18.926768: step 11700, loss 0.0811426, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-11700\n",
      "\n",
      "2017-11-26T18:31:46.156789: step 11800, loss 0.0867956, acc 1\n",
      "2017-11-26T18:32:11.703064: step 11900, loss 0.100901, acc 1\n",
      "2017-11-26T18:32:37.434592: step 12000, loss 0.102426, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-12000\n",
      "\n",
      "2017-11-26T18:33:04.606764: step 12100, loss 0.0968536, acc 1\n",
      "2017-11-26T18:33:30.308946: step 12200, loss 0.0867942, acc 1\n",
      "2017-11-26T18:33:55.940001: step 12300, loss 0.0851889, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-12300\n",
      "\n",
      "2017-11-26T18:34:23.094240: step 12400, loss 0.161712, acc 1\n",
      "2017-11-26T18:34:48.721328: step 12500, loss 0.0882915, acc 1\n",
      "2017-11-26T18:35:14.377322: step 12600, loss 0.104435, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-12600\n",
      "\n",
      "2017-11-26T18:35:41.479653: step 12700, loss 0.112284, acc 1\n",
      "2017-11-26T18:36:07.117729: step 12800, loss 0.0809842, acc 1\n",
      "2017-11-26T18:36:32.682382: step 12900, loss 0.0722889, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-12900\n",
      "\n",
      "2017-11-26T18:36:59.837008: step 13000, loss 0.0785743, acc 1\n",
      "2017-11-26T18:37:25.441980: step 13100, loss 0.0857714, acc 1\n",
      "2017-11-26T18:37:51.060875: step 13200, loss 0.109249, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-13200\n",
      "\n",
      "2017-11-26T18:38:18.190949: step 13300, loss 0.0893888, acc 1\n",
      "2017-11-26T18:38:43.851719: step 13400, loss 0.0956878, acc 1\n",
      "2017-11-26T18:39:09.464301: step 13500, loss 0.133562, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-13500\n",
      "\n",
      "2017-11-26T18:39:36.469832: step 13600, loss 0.0847085, acc 1\n",
      "2017-11-26T18:40:02.055485: step 13700, loss 0.0678926, acc 1\n",
      "2017-11-26T18:40:27.665437: step 13800, loss 0.0827433, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-13800\n",
      "\n",
      "2017-11-26T18:40:54.677378: step 13900, loss 0.116457, acc 0.96875\n",
      "2017-11-26T18:41:20.296086: step 14000, loss 0.086512, acc 1\n",
      "2017-11-26T18:41:45.923145: step 14100, loss 0.0795208, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-14100\n",
      "\n",
      "2017-11-26T18:42:13.172694: step 14200, loss 0.119296, acc 0.96875\n",
      "2017-11-26T18:42:38.777033: step 14300, loss 0.0695906, acc 1\n",
      "2017-11-26T18:43:04.424582: step 14400, loss 0.0705355, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-14400\n",
      "\n",
      "2017-11-26T18:43:31.580810: step 14500, loss 0.084186, acc 1\n",
      "2017-11-26T18:43:57.159310: step 14600, loss 0.0647206, acc 1\n",
      "2017-11-26T18:44:22.845668: step 14700, loss 0.0751148, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-14700\n",
      "\n",
      "2017-11-26T18:44:49.871390: step 14800, loss 0.0676362, acc 1\n",
      "2017-11-26T18:45:15.526971: step 14900, loss 0.0802673, acc 1\n",
      "2017-11-26T18:45:41.163088: step 15000, loss 0.168891, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-15000\n",
      "\n",
      "2017-11-26T18:46:08.358196: step 15100, loss 0.0590668, acc 1\n",
      "2017-11-26T18:46:34.051443: step 15200, loss 0.0669438, acc 1\n",
      "2017-11-26T18:46:59.803476: step 15300, loss 0.0573961, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-15300\n",
      "\n",
      "2017-11-26T18:47:26.956957: step 15400, loss 0.0668452, acc 1\n",
      "2017-11-26T18:47:52.652466: step 15500, loss 0.173466, acc 0.96875\n",
      "2017-11-26T18:48:18.326555: step 15600, loss 0.0666177, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-15600\n",
      "\n",
      "2017-11-26T18:48:45.423267: step 15700, loss 0.0695199, acc 1\n",
      "2017-11-26T18:49:11.148215: step 15800, loss 0.114543, acc 1\n",
      "2017-11-26T18:49:36.842378: step 15900, loss 0.0656659, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-15900\n",
      "\n",
      "2017-11-26T18:50:03.979578: step 16000, loss 0.0942519, acc 1\n",
      "2017-11-26T18:50:29.662115: step 16100, loss 0.0540376, acc 1\n",
      "2017-11-26T18:50:55.345969: step 16200, loss 0.0625348, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-16200\n",
      "\n",
      "2017-11-26T18:51:23.507706: step 16300, loss 0.0492787, acc 1\n",
      "2017-11-26T18:51:49.140925: step 16400, loss 0.0590393, acc 1\n",
      "2017-11-26T18:52:14.899114: step 16500, loss 0.0516018, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-16500\n",
      "\n",
      "2017-11-26T18:52:42.098808: step 16600, loss 0.0480514, acc 1\n",
      "2017-11-26T18:53:07.725851: step 16700, loss 0.227386, acc 0.96875\n",
      "2017-11-26T18:53:33.366263: step 16800, loss 0.0662332, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-16800\n",
      "\n",
      "2017-11-26T18:54:00.541209: step 16900, loss 0.0551622, acc 1\n",
      "2017-11-26T18:54:26.157666: step 17000, loss 0.0608119, acc 1\n",
      "2017-11-26T18:54:51.835801: step 17100, loss 0.0484488, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-17100\n",
      "\n",
      "2017-11-26T18:55:19.077374: step 17200, loss 0.0888502, acc 0.96875\n",
      "2017-11-26T18:55:44.808240: step 17300, loss 0.0487528, acc 1\n",
      "2017-11-26T18:56:10.564280: step 17400, loss 0.046642, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-17400\n",
      "\n",
      "2017-11-26T18:56:37.754077: step 17500, loss 0.0523898, acc 1\n",
      "2017-11-26T18:57:03.640568: step 17600, loss 0.045033, acc 1\n",
      "2017-11-26T18:57:29.438420: step 17700, loss 0.0652759, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-17700\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-17700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step%100 == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def dev_step(x_batch, y_batch, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        print('Evaluation....')\n",
    "        loops = int(len(x_batch)/32)\n",
    "        remains = len(x_batch) - 32*loops\n",
    "        count = 0\n",
    "        for i in range(loops):\n",
    "            start = i * 32\n",
    "            end = (i+1) * 32\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        for i in range(remains):\n",
    "            start = 32 * loops + i\n",
    "            end = 32 * loops + i + 1\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, float(correct_num)/len(y_batch)))\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "    # Generate batches\n",
    "    batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), 32, 50)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "        \n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 300 == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "    print(\"Saved model checkpoint to {}\\n\".format(path))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    model_file=tf.train.latest_checkpoint('runs/1509495505/checkpoints/')\n",
    "    saver.restore(sess, model_file)\n",
    "    # Generate batches\n",
    "    batches = data_helpers.batch_iter(\n",
    "           list(zip(x_train, y_train)), 32, 10)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "        \n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 300 == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "    print(\"Saved model checkpoint to {}\\n\".format(path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1511689130\\checkpoints\\model-17700\n",
      "Accuracy:0.81452\n"
     ]
    }
   ],
   "source": [
    "#Restore the parameters and do testing\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    model_file=tf.train.latest_checkpoint('runs/1511689130/checkpoints/')\n",
    "    saver.restore(sess, model_file)\n",
    "    loops = int(len(x_test)/32)\n",
    "    remains = len(x_test) - 32*loops\n",
    "    count = 0\n",
    "    for i in range(loops):\n",
    "        start = i * 32\n",
    "        end = (i+1) * 32\n",
    "        x = x_test[start: end]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x,\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "        correct_num = sess.run(cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "        \n",
    "    for i in range(remains):\n",
    "        start = 32 * loops + i\n",
    "        end = 32 * loops + i + 1\n",
    "        x = x_test[start: ]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x,\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        correct_num = sess.run(cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"Accuracy:{:.5f}\".format(float(count)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "注意进入路径下 tensorboard --logir=train 注意不要空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
