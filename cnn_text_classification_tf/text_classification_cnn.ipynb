{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment aims to classifiy 20newsgroup through convolutional neural network models. Based on the original [paper](http://arxiv.org/abs/1408.5882), [dennybritz](https://github.com/dennybritz/cnn-text-classification-tf) realized this model on sentimental classification using tensorflow. And he was generous to share his work at [Github](https://github.com/dennybritz/cnn-text-classification-tf). Refering to his work,  I made some modifications on the model and classify 20newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Texts\n",
    "I have preprocess the original texts and saved them as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "test_data = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in train_data.text])\n",
    "#Cut long articles to 800 words. Pad short ones\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(800)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(list(vocab_processor.transform(test_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_data.target, test_data.target\n",
    "y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "y_test = np.array(y_test).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encode the label as one-hot code\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y_train = ohe.fit_transform(y_train)\n",
    "y_test = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Restore the values from sparse matrix\n",
    "y_train = np.array([item.toarray().reshape(-1) for item in y_train])\n",
    "y_test = np.array([item.toarray().reshape(-1) for item in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=64,\n",
    "            filter_sizes= [3, 4, 5, 6],\n",
    "            num_filters=32,\n",
    "            l2_reg_lambda=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #train_op = optimizer.minimize(cnn.loss)\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "    \n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    for g, v in grads_and_vars:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-01T08:19:19.986761: step 100, loss 4.95034, acc 0.09375\n",
      "2017-11-01T08:19:44.527734: step 200, loss 4.74812, acc 0.09375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-200\n",
      "\n",
      "2017-11-01T08:20:10.927117: step 300, loss 3.64749, acc 0.09375\n",
      "2017-11-01T08:20:35.603756: step 400, loss 3.72247, acc 0.09375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-400\n",
      "\n",
      "2017-11-01T08:21:01.370469: step 500, loss 3.69736, acc 0.0625\n",
      "2017-11-01T08:21:25.801734: step 600, loss 2.91698, acc 0.1875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-600\n",
      "\n",
      "2017-11-01T08:21:51.764804: step 700, loss 3.23904, acc 0.0625\n",
      "2017-11-01T08:22:16.486764: step 800, loss 2.8626, acc 0.21875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-800\n",
      "\n",
      "2017-11-01T08:22:42.832698: step 900, loss 2.56613, acc 0.25\n",
      "2017-11-01T08:23:07.294910: step 1000, loss 2.27068, acc 0.34375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-01T08:23:33.158780: step 1100, loss 2.27068, acc 0.28125\n",
      "2017-11-01T08:23:57.720836: step 1200, loss 2.26938, acc 0.375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-01T08:24:23.581829: step 1300, loss 2.04727, acc 0.46875\n",
      "2017-11-01T08:24:48.072274: step 1400, loss 2.67178, acc 0.3125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-01T08:25:13.889601: step 1500, loss 2.19897, acc 0.375\n",
      "2017-11-01T08:25:38.204193: step 1600, loss 1.6087, acc 0.625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-01T08:26:04.269938: step 1700, loss 1.97096, acc 0.40625\n",
      "2017-11-01T08:26:28.830595: step 1800, loss 1.60402, acc 0.5\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-01T08:26:54.860394: step 1900, loss 1.63743, acc 0.53125\n",
      "2017-11-01T08:27:19.304201: step 2000, loss 1.50513, acc 0.6875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-01T08:27:45.321780: step 2100, loss 1.79336, acc 0.625\n",
      "2017-11-01T08:28:09.845179: step 2200, loss 1.73165, acc 0.625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-01T08:28:35.844714: step 2300, loss 1.20975, acc 0.65625\n",
      "2017-11-01T08:29:00.206173: step 2400, loss 1.37161, acc 0.625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-01T08:29:26.230594: step 2500, loss 1.06229, acc 0.71875\n",
      "2017-11-01T08:29:50.725832: step 2600, loss 1.37807, acc 0.71875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-01T08:30:16.740297: step 2700, loss 0.866747, acc 0.8125\n",
      "2017-11-01T08:30:41.317915: step 2800, loss 1.34597, acc 0.5625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-01T08:31:07.398946: step 2900, loss 1.26403, acc 0.6875\n",
      "2017-11-01T08:31:31.897302: step 3000, loss 0.816143, acc 0.875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-01T08:31:57.922166: step 3100, loss 0.83692, acc 0.8125\n",
      "2017-11-01T08:32:22.195796: step 3200, loss 0.634396, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-01T08:32:47.510500: step 3300, loss 0.860151, acc 0.84375\n",
      "2017-11-01T08:33:11.546301: step 3400, loss 0.989661, acc 0.75\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-01T08:33:37.128336: step 3500, loss 1.1966, acc 0.78125\n",
      "2017-11-01T08:34:01.362286: step 3600, loss 1.05407, acc 0.75\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-3600\n",
      "\n",
      "2017-11-01T08:34:26.801556: step 3700, loss 0.633862, acc 0.9375\n",
      "2017-11-01T08:34:50.817010: step 3800, loss 0.803215, acc 0.8125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-3800\n",
      "\n",
      "2017-11-01T08:35:16.296079: step 3900, loss 0.560653, acc 0.90625\n",
      "2017-11-01T08:35:40.317081: step 4000, loss 0.709452, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-4000\n",
      "\n",
      "2017-11-01T08:36:05.818689: step 4100, loss 0.573147, acc 0.90625\n",
      "2017-11-01T08:36:29.942159: step 4200, loss 0.472765, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-4200\n",
      "\n",
      "2017-11-01T08:36:55.352598: step 4300, loss 0.569219, acc 0.9375\n",
      "2017-11-01T08:37:19.483145: step 4400, loss 0.456787, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-4400\n",
      "\n",
      "2017-11-01T08:37:45.193247: step 4500, loss 0.707035, acc 0.9375\n",
      "2017-11-01T08:38:09.365416: step 4600, loss 0.406851, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-4600\n",
      "\n",
      "2017-11-01T08:38:35.002749: step 4700, loss 0.486666, acc 1\n",
      "2017-11-01T08:38:59.196696: step 4800, loss 0.570353, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-4800\n",
      "\n",
      "2017-11-01T08:39:24.626459: step 4900, loss 0.458728, acc 0.96875\n",
      "2017-11-01T08:39:48.654140: step 5000, loss 0.554067, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-5000\n",
      "\n",
      "2017-11-01T08:40:14.310982: step 5100, loss 0.364769, acc 0.96875\n",
      "2017-11-01T08:40:38.515817: step 5200, loss 0.444086, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-5200\n",
      "\n",
      "2017-11-01T08:41:04.071286: step 5300, loss 0.353723, acc 1\n",
      "2017-11-01T08:41:28.207458: step 5400, loss 0.413832, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-5400\n",
      "\n",
      "2017-11-01T08:41:53.724027: step 5500, loss 0.466064, acc 1\n",
      "2017-11-01T08:42:17.909772: step 5600, loss 0.336086, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-5600\n",
      "\n",
      "2017-11-01T08:42:43.293192: step 5700, loss 0.434546, acc 0.90625\n",
      "2017-11-01T08:43:07.723475: step 5800, loss 0.389995, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-5800\n",
      "\n",
      "2017-11-01T08:43:34.738073: step 5900, loss 0.509417, acc 0.90625\n",
      "2017-11-01T08:43:59.577823: step 6000, loss 0.357706, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-6000\n",
      "\n",
      "2017-11-01T08:44:25.510588: step 6100, loss 0.319766, acc 1\n",
      "2017-11-01T08:44:50.070227: step 6200, loss 0.444512, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-6200\n",
      "\n",
      "2017-11-01T08:45:16.066679: step 6300, loss 0.326795, acc 1\n",
      "2017-11-01T08:45:40.647519: step 6400, loss 0.467872, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-6400\n",
      "\n",
      "2017-11-01T08:46:06.690948: step 6500, loss 0.257302, acc 1\n",
      "2017-11-01T08:46:31.274715: step 6600, loss 0.367854, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-6600\n",
      "\n",
      "2017-11-01T08:46:57.155519: step 6700, loss 0.361232, acc 0.9375\n",
      "2017-11-01T08:47:21.760884: step 6800, loss 0.31226, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-6800\n",
      "\n",
      "2017-11-01T08:47:47.957247: step 6900, loss 0.342883, acc 0.9375\n",
      "2017-11-01T08:48:12.549942: step 7000, loss 0.259724, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-7000\n",
      "\n",
      "2017-11-01T08:48:38.583617: step 7100, loss 0.191577, acc 1\n",
      "2017-11-01T08:49:03.144705: step 7200, loss 0.319379, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-7200\n",
      "\n",
      "2017-11-01T08:49:29.170067: step 7300, loss 0.225649, acc 1\n",
      "2017-11-01T08:49:53.716527: step 7400, loss 0.253681, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-7400\n",
      "\n",
      "2017-11-01T08:50:19.709984: step 7500, loss 0.240883, acc 1\n",
      "2017-11-01T08:50:44.295666: step 7600, loss 0.241623, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-7600\n",
      "\n",
      "2017-11-01T08:51:10.377186: step 7700, loss 0.304563, acc 0.96875\n",
      "2017-11-01T08:51:35.055161: step 7800, loss 0.248317, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-7800\n",
      "\n",
      "2017-11-01T08:52:01.089558: step 7900, loss 0.174361, acc 1\n",
      "2017-11-01T08:52:25.751208: step 8000, loss 0.232409, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-8000\n",
      "\n",
      "2017-11-01T08:52:51.814662: step 8100, loss 0.264005, acc 0.96875\n",
      "2017-11-01T08:53:16.340571: step 8200, loss 0.557382, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-8200\n",
      "\n",
      "2017-11-01T08:53:41.939469: step 8300, loss 0.306982, acc 0.96875\n",
      "2017-11-01T08:54:06.030458: step 8400, loss 0.19738, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-8400\n",
      "\n",
      "2017-11-01T08:54:31.511516: step 8500, loss 0.278461, acc 0.9375\n",
      "2017-11-01T08:54:55.607638: step 8600, loss 0.21755, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-8600\n",
      "\n",
      "2017-11-01T08:55:21.254415: step 8700, loss 0.226714, acc 0.96875\n",
      "2017-11-01T08:55:45.437514: step 8800, loss 0.158954, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-8800\n",
      "\n",
      "2017-11-01T08:56:10.712558: step 8900, loss 0.230062, acc 0.9375\n",
      "2017-11-01T08:56:34.961730: step 9000, loss 0.189983, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-9000\n",
      "\n",
      "2017-11-01T08:57:00.434920: step 9100, loss 0.191956, acc 0.96875\n",
      "2017-11-01T08:57:24.572005: step 9200, loss 0.271751, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-9200\n",
      "\n",
      "2017-11-01T08:57:50.149164: step 9300, loss 0.212553, acc 0.96875\n",
      "2017-11-01T08:58:14.351969: step 9400, loss 0.399087, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-9400\n",
      "\n",
      "2017-11-01T08:58:39.992480: step 9500, loss 0.170572, acc 1\n",
      "2017-11-01T08:59:04.264377: step 9600, loss 0.171692, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-9600\n",
      "\n",
      "2017-11-01T08:59:29.743941: step 9700, loss 0.209455, acc 0.96875\n",
      "2017-11-01T08:59:53.859356: step 9800, loss 0.278754, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-9800\n",
      "\n",
      "2017-11-01T09:00:19.574959: step 9900, loss 0.135402, acc 1\n",
      "2017-11-01T09:00:43.783771: step 10000, loss 0.126672, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-10000\n",
      "\n",
      "2017-11-01T09:01:09.398900: step 10100, loss 0.140477, acc 1\n",
      "2017-11-01T09:01:33.374678: step 10200, loss 0.217227, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-10200\n",
      "\n",
      "2017-11-01T09:01:59.094510: step 10300, loss 0.142882, acc 1\n",
      "2017-11-01T09:02:23.169521: step 10400, loss 0.215911, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-10400\n",
      "\n",
      "2017-11-01T09:02:48.561994: step 10500, loss 0.127732, acc 1\n",
      "2017-11-01T09:03:12.740492: step 10600, loss 0.162064, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-10600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step%100 == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def dev_step(x_batch, y_batch, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        print('Evaluation....')\n",
    "        loops = int(len(x_batch)/32)\n",
    "        remains = len(x_batch) - 32*loops\n",
    "        count = 0\n",
    "        for i in range(loops):\n",
    "            start = i * 32\n",
    "            end = (i+1) * 32\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        for i in range(remains):\n",
    "            start = 32 * loops + i\n",
    "            end = 32 * loops + i + 1\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, float(correct_num)/len(y_batch)))\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "    # Generate batches\n",
    "    batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), 32, 50)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "        \n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 300 == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "    print(\"Saved model checkpoint to {}\\n\".format(path))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-10600\n",
      "2017-11-01T10:53:14.178573: step 10700, loss 0.130815, acc 1\n",
      "2017-11-01T10:53:38.938346: step 10800, loss 0.125105, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-10800\n",
      "\n",
      "2017-11-01T10:54:05.163908: step 10900, loss 0.154567, acc 1\n",
      "2017-11-01T10:54:29.679825: step 11000, loss 0.15521, acc 1\n",
      "2017-11-01T10:54:54.389944: step 11100, loss 0.241976, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-11100\n",
      "\n",
      "2017-11-01T10:55:20.553063: step 11200, loss 0.162005, acc 1\n",
      "2017-11-01T10:55:45.300993: step 11300, loss 0.128682, acc 1\n",
      "2017-11-01T10:56:09.944910: step 11400, loss 0.135597, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-11400\n",
      "\n",
      "2017-11-01T10:56:35.963191: step 11500, loss 0.0995723, acc 1\n",
      "2017-11-01T10:57:00.640187: step 11600, loss 0.186918, acc 0.96875\n",
      "2017-11-01T10:57:25.282846: step 11700, loss 0.108883, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-11700\n",
      "\n",
      "2017-11-01T10:57:51.520750: step 11800, loss 0.184686, acc 0.9375\n",
      "2017-11-01T10:58:16.628437: step 11900, loss 0.114718, acc 1\n",
      "2017-11-01T10:58:41.888123: step 12000, loss 0.122167, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-12000\n",
      "\n",
      "2017-11-01T10:59:08.138528: step 12100, loss 0.11127, acc 1\n",
      "2017-11-01T10:59:32.924682: step 12200, loss 0.11717, acc 1\n",
      "2017-11-01T10:59:57.593231: step 12300, loss 0.116741, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-12300\n",
      "\n",
      "2017-11-01T11:00:23.793189: step 12400, loss 0.111692, acc 1\n",
      "2017-11-01T11:00:48.424092: step 12500, loss 0.173896, acc 0.96875\n",
      "2017-11-01T11:01:13.196141: step 12600, loss 0.121103, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-12600\n",
      "\n",
      "2017-11-01T11:01:39.287630: step 12700, loss 0.148233, acc 0.96875\n",
      "2017-11-01T11:02:03.993873: step 12800, loss 0.0863076, acc 1\n",
      "2017-11-01T11:02:28.855074: step 12900, loss 0.0832964, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-12900\n",
      "\n",
      "2017-11-01T11:02:55.137487: step 13000, loss 0.098657, acc 1\n",
      "2017-11-01T11:03:19.830099: step 13100, loss 0.101929, acc 1\n",
      "2017-11-01T11:03:44.760998: step 13200, loss 0.110457, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-13200\n",
      "\n",
      "2017-11-01T11:04:11.219508: step 13300, loss 0.120317, acc 0.96875\n",
      "2017-11-01T11:04:35.860991: step 13400, loss 0.0959118, acc 1\n",
      "2017-11-01T11:05:00.801226: step 13500, loss 0.116481, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-13500\n",
      "\n",
      "2017-11-01T11:05:27.151304: step 13600, loss 0.0933297, acc 1\n",
      "2017-11-01T11:05:52.187026: step 13700, loss 0.0768865, acc 1\n",
      "2017-11-01T11:06:17.285148: step 13800, loss 0.0884602, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-13800\n",
      "\n",
      "2017-11-01T11:06:43.511938: step 13900, loss 0.104534, acc 1\n",
      "2017-11-01T11:07:08.214081: step 14000, loss 0.0789726, acc 1\n",
      "2017-11-01T11:07:33.041196: step 14100, loss 0.0919228, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-14100\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-14140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    model_file=tf.train.latest_checkpoint('runs/1509495505/checkpoints/')\n",
    "    saver.restore(sess, model_file)\n",
    "    # Generate batches\n",
    "    batches = data_helpers.batch_iter(\n",
    "           list(zip(x_train, y_train)), 32, 10)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "        \n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 300 == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "    print(\"Saved model checkpoint to {}\\n\".format(path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509495505\\checkpoints\\model-14140\n",
      "Accuracy:0.79607\n"
     ]
    }
   ],
   "source": [
    "#Restore the parameters and do testing\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    model_file=tf.train.latest_checkpoint('runs/1509495505/checkpoints/')\n",
    "    saver.restore(sess, model_file)\n",
    "    loops = int(len(x_test)/32)\n",
    "    remains = len(x_test) - 32*loops\n",
    "    count = 0\n",
    "    for i in range(loops):\n",
    "        start = i * 32\n",
    "        end = (i+1) * 32\n",
    "        x = x_test[start: end]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x,\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "        correct_num = sess.run(cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "        \n",
    "    for i in range(remains):\n",
    "        start = 32 * loops + i\n",
    "        end = 32 * loops + i + 1\n",
    "        x = x_test[start: ]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x,\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        correct_num = sess.run(cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"Accuracy:{:.5f}\".format(float(count)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "注意进入路径下 tensorboard --logir=train 注意不要空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
