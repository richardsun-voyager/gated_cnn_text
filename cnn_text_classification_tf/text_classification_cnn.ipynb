{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment aims to classifiy 20newsgroup through convolutional neural network models. Based on the original [paper](http://arxiv.org/abs/1408.5882), [dennybritz](https://github.com/dennybritz/cnn-text-classification-tf) realized this model on sentimental classification using tensorflow. And he was generous to share his work at [Github](https://github.com/dennybritz/cnn-text-classification-tf). Refering to his work,  I made some modifications on the model and classify 20newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Texts\n",
    "I have preprocess the original texts and saved them as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "test_data = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in train_data.text])\n",
    "#Cut long articles to 800 words. Pad short ones\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(800)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(list(vocab_processor.transform(test_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_data.target, test_data.target\n",
    "y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "y_test = np.array(y_test).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encode the label as one-hot code\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y_train = ohe.fit_transform(y_train)\n",
    "y_test = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Restore the values from sparse matrix\n",
    "y_train = np.array([item.toarray().reshape(-1) for item in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.array([item.toarray().reshape(-1) for item in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=64,\n",
    "            filter_sizes= [3, 4, 5, 6],\n",
    "            num_filters=32,\n",
    "            l2_reg_lambda=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #train_op = optimizer.minimize(cnn.loss)\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "    \n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    for g, v in grads_and_vars:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-31T20:37:50.639686: step 100, loss 5.16044, acc 0.03125\n",
      "2017-10-31T20:38:15.776140: step 200, loss 4.82593, acc 0.03125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-200\n",
      "\n",
      "2017-10-31T20:38:42.303047: step 300, loss 3.74404, acc 0.0625\n",
      "2017-10-31T20:39:07.492546: step 400, loss 3.2584, acc 0.15625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-400\n",
      "\n",
      "2017-10-31T20:39:33.995184: step 500, loss 3.3786, acc 0.0625\n",
      "2017-10-31T20:39:58.830886: step 600, loss 3.09255, acc 0.15625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-600\n",
      "\n",
      "2017-10-31T20:40:25.098630: step 700, loss 2.89328, acc 0.21875\n",
      "2017-10-31T20:40:49.891069: step 800, loss 2.58475, acc 0.21875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-800\n",
      "\n",
      "2017-10-31T20:41:17.696245: step 900, loss 2.69883, acc 0.15625\n",
      "2017-10-31T20:41:43.884825: step 1000, loss 2.4728, acc 0.25\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-1000\n",
      "\n",
      "2017-10-31T20:42:10.547780: step 1100, loss 2.29659, acc 0.46875\n",
      "2017-10-31T20:42:35.973278: step 1200, loss 2.16409, acc 0.40625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-1200\n",
      "\n",
      "2017-10-31T20:43:03.298728: step 1300, loss 2.46137, acc 0.34375\n",
      "2017-10-31T20:43:28.382762: step 1400, loss 2.42111, acc 0.34375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-1400\n",
      "\n",
      "2017-10-31T20:43:55.419480: step 1500, loss 2.51632, acc 0.28125\n",
      "2017-10-31T20:44:20.658793: step 1600, loss 2.22181, acc 0.25\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-1600\n",
      "\n",
      "2017-10-31T20:44:48.167054: step 1700, loss 2.20627, acc 0.40625\n",
      "2017-10-31T20:45:13.606080: step 1800, loss 1.5359, acc 0.625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-1800\n",
      "\n",
      "2017-10-31T20:45:40.359571: step 1900, loss 1.72863, acc 0.53125\n",
      "2017-10-31T20:46:05.270288: step 2000, loss 1.89074, acc 0.5\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-2000\n",
      "\n",
      "2017-10-31T20:46:31.403395: step 2100, loss 1.16914, acc 0.625\n",
      "2017-10-31T20:46:56.023989: step 2200, loss 1.76419, acc 0.46875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-2200\n",
      "\n",
      "2017-10-31T20:47:22.086005: step 2300, loss 1.37881, acc 0.65625\n",
      "2017-10-31T20:47:46.445609: step 2400, loss 1.36858, acc 0.75\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-2400\n",
      "\n",
      "2017-10-31T20:48:12.327952: step 2500, loss 1.09699, acc 0.8125\n",
      "2017-10-31T20:48:37.003866: step 2600, loss 1.18751, acc 0.75\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-2600\n",
      "\n",
      "2017-10-31T20:49:03.153791: step 2700, loss 1.40742, acc 0.65625\n",
      "2017-10-31T20:49:27.848574: step 2800, loss 1.3486, acc 0.6875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-2800\n",
      "\n",
      "2017-10-31T20:49:53.528649: step 2900, loss 1.05233, acc 0.875\n",
      "2017-10-31T20:50:18.103470: step 3000, loss 0.72456, acc 0.875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-3000\n",
      "\n",
      "2017-10-31T20:50:44.604124: step 3100, loss 1.42532, acc 0.625\n",
      "2017-10-31T20:51:09.326152: step 3200, loss 0.880591, acc 0.84375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-3200\n",
      "\n",
      "2017-10-31T20:51:35.358931: step 3300, loss 0.763155, acc 0.90625\n",
      "2017-10-31T20:52:00.004309: step 3400, loss 0.746951, acc 0.78125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-3400\n",
      "\n",
      "2017-10-31T20:52:25.885295: step 3500, loss 0.930041, acc 0.84375\n",
      "2017-10-31T20:52:50.531158: step 3600, loss 0.630153, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-3600\n",
      "\n",
      "2017-10-31T20:53:16.678946: step 3700, loss 1.10281, acc 0.875\n",
      "2017-10-31T20:53:41.340193: step 3800, loss 0.879506, acc 0.8125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-3800\n",
      "\n",
      "2017-10-31T20:54:07.383084: step 3900, loss 0.639872, acc 0.875\n",
      "2017-10-31T20:54:32.016010: step 4000, loss 0.714547, acc 0.84375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-4000\n",
      "\n",
      "2017-10-31T20:54:58.149609: step 4100, loss 0.60612, acc 0.96875\n",
      "2017-10-31T20:55:22.728006: step 4200, loss 0.738036, acc 0.84375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-4200\n",
      "\n",
      "2017-10-31T20:55:48.808082: step 4300, loss 0.553676, acc 0.90625\n",
      "2017-10-31T20:56:13.001298: step 4400, loss 0.570075, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-4400\n",
      "\n",
      "2017-10-31T20:56:38.365823: step 4500, loss 0.641094, acc 0.9375\n",
      "2017-10-31T20:57:02.614873: step 4600, loss 0.46508, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-4600\n",
      "\n",
      "2017-10-31T20:57:28.354744: step 4700, loss 0.668163, acc 0.90625\n",
      "2017-10-31T20:57:52.591480: step 4800, loss 0.566419, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-4800\n",
      "\n",
      "2017-10-31T20:58:18.086700: step 4900, loss 0.286435, acc 1\n",
      "2017-10-31T20:58:42.308696: step 5000, loss 0.529854, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-5000\n",
      "\n",
      "2017-10-31T20:59:07.636083: step 5100, loss 0.266154, acc 1\n",
      "2017-10-31T20:59:31.888835: step 5200, loss 0.507255, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-5200\n",
      "\n",
      "2017-10-31T20:59:57.373274: step 5300, loss 0.324819, acc 0.96875\n",
      "2017-10-31T21:00:21.589341: step 5400, loss 0.484745, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-5400\n",
      "\n",
      "2017-10-31T21:00:47.303216: step 5500, loss 0.281435, acc 1\n",
      "2017-10-31T21:01:11.432076: step 5600, loss 0.307881, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-5600\n",
      "\n",
      "2017-10-31T21:01:36.898195: step 5700, loss 0.320453, acc 1\n",
      "2017-10-31T21:02:01.169305: step 5800, loss 0.359875, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-5800\n",
      "\n",
      "2017-10-31T21:02:26.733908: step 5900, loss 0.374278, acc 0.9375\n",
      "2017-10-31T21:02:50.877200: step 6000, loss 0.399315, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-6000\n",
      "\n",
      "2017-10-31T21:03:16.385482: step 6100, loss 0.336715, acc 0.96875\n",
      "2017-10-31T21:03:40.627578: step 6200, loss 0.334553, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-6200\n",
      "\n",
      "2017-10-31T21:04:06.043672: step 6300, loss 0.293815, acc 1\n",
      "2017-10-31T21:04:30.143600: step 6400, loss 0.34007, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-6400\n",
      "\n",
      "2017-10-31T21:04:55.969168: step 6500, loss 0.30645, acc 1\n",
      "2017-10-31T21:05:20.196531: step 6600, loss 0.361203, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-6600\n",
      "\n",
      "2017-10-31T21:05:45.802931: step 6700, loss 0.304333, acc 0.96875\n",
      "2017-10-31T21:06:09.978695: step 6800, loss 0.29553, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-6800\n",
      "\n",
      "2017-10-31T21:06:35.468820: step 6900, loss 0.201775, acc 1\n",
      "2017-10-31T21:06:59.541881: step 7000, loss 0.218405, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-7000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step%100 == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def dev_step(x_batch, y_batch, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        print('Evaluation....')\n",
    "        loops = int(len(x_batch)/32)\n",
    "        remains = len(x_batch) - 32*loops\n",
    "        count = 0\n",
    "        for i in range(loops):\n",
    "            start = i * 32\n",
    "            end = (i+1) * 32\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        for i in range(remains):\n",
    "            start = 32 * loops + i\n",
    "            end = 32 * loops + i + 1\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, float(correct_num)/len(y_batch)))\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "    # Generate batches\n",
    "    batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), 32, 20)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "        \n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 200 == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn_text_classification_tf\\runs\\1509453435\\checkpoints\\model-7000\n",
      "Accuracy:0.78014\n"
     ]
    }
   ],
   "source": [
    "#Restore the parameters and do testing\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    model_file=tf.train.latest_checkpoint('runs/1509453435/checkpoints/')\n",
    "    saver.restore(sess, model_file)\n",
    "    loops = int(len(x_test)/32)\n",
    "    remains = len(x_test) - 32*loops\n",
    "    count = 0\n",
    "    for i in range(loops):\n",
    "        start = i * 32\n",
    "        end = (i+1) * 32\n",
    "        x = x_test[start: end]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x,\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "        correct_num = sess.run(cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "    for i in range(remains):\n",
    "        start = 32 * loops + i\n",
    "        end = 32 * loops + i + 1\n",
    "        x = x_test[start: ]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x,\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        correct_num = sess.run(cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"Accuracy:{:.5f}\".format(float(count)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "注意进入路径下 tensorboard --logir=train 注意不要空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
