{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "test_data = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in train_data.text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(800)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(list(vocab_processor.transform(test_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_data.target, test_data.target\n",
    "y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "y_test = np.array(y_test).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encode the label as one-hot code\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y_train = ohe.fit_transform(y_train)\n",
    "y_test = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.array([item.toarray().reshape(-1) for item in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=64,\n",
    "            filter_sizes= [3, 4, 5],\n",
    "            num_filters=32,\n",
    "            l2_reg_lambda=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509287411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "    \n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    for g, v in grads_and_vars:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-29T22:30:16.726902: step 1, loss 9.68835, acc 0.03125\n",
      "2017-10-29T22:30:16.978845: step 2, loss 9.62094, acc 0\n",
      "2017-10-29T22:30:17.212626: step 3, loss 8.44433, acc 0.03125\n",
      "2017-10-29T22:30:17.444415: step 4, loss 7.79644, acc 0.03125\n",
      "2017-10-29T22:30:17.682706: step 5, loss 8.5534, acc 0\n",
      "2017-10-29T22:30:17.914489: step 6, loss 8.03319, acc 0.0625\n",
      "2017-10-29T22:30:18.146274: step 7, loss 7.54005, acc 0.0625\n",
      "2017-10-29T22:30:18.384579: step 8, loss 8.47827, acc 0\n",
      "2017-10-29T22:30:18.616358: step 9, loss 8.35973, acc 0.03125\n",
      "2017-10-29T22:30:18.863631: step 10, loss 8.6254, acc 0.09375\n",
      "2017-10-29T22:30:19.101915: step 11, loss 8.06973, acc 0.03125\n",
      "2017-10-29T22:30:19.333705: step 12, loss 9.49952, acc 0\n",
      "2017-10-29T22:30:19.565485: step 13, loss 8.04633, acc 0.03125\n",
      "2017-10-29T22:30:19.803545: step 14, loss 7.31617, acc 0.03125\n",
      "2017-10-29T22:30:20.035325: step 15, loss 8.90434, acc 0.09375\n",
      "2017-10-29T22:30:20.286243: step 16, loss 8.21378, acc 0.09375\n",
      "2017-10-29T22:30:20.505390: step 17, loss 9.43155, acc 0\n",
      "2017-10-29T22:30:20.752801: step 18, loss 8.84543, acc 0.03125\n",
      "2017-10-29T22:30:20.990588: step 19, loss 8.42392, acc 0.0625\n",
      "2017-10-29T22:30:21.222929: step 20, loss 7.84397, acc 0.0625\n",
      "2017-10-29T22:30:21.470365: step 21, loss 8.96399, acc 0.03125\n",
      "2017-10-29T22:30:21.708652: step 22, loss 7.60707, acc 0.0625\n",
      "2017-10-29T22:30:21.940457: step 23, loss 8.65428, acc 0.0625\n",
      "2017-10-29T22:30:22.172242: step 24, loss 9.05268, acc 0\n",
      "2017-10-29T22:30:22.394903: step 25, loss 8.7183, acc 0.0625\n",
      "2017-10-29T22:30:22.626681: step 26, loss 7.09464, acc 0.09375\n",
      "2017-10-29T22:30:22.858469: step 27, loss 7.34355, acc 0\n",
      "2017-10-29T22:30:23.096755: step 28, loss 9.10933, acc 0.03125\n",
      "2017-10-29T22:30:23.312912: step 29, loss 9.58601, acc 0.0625\n",
      "2017-10-29T22:30:23.560330: step 30, loss 7.48799, acc 0.0625\n",
      "2017-10-29T22:30:23.796132: step 31, loss 8.00987, acc 0\n",
      "2017-10-29T22:30:24.014786: step 32, loss 7.61558, acc 0.09375\n",
      "2017-10-29T22:30:24.246589: step 33, loss 7.45186, acc 0\n",
      "2017-10-29T22:30:24.493685: step 34, loss 6.86043, acc 0.125\n",
      "2017-10-29T22:30:24.716348: step 35, loss 8.39993, acc 0\n",
      "2017-10-29T22:30:24.948149: step 36, loss 8.73956, acc 0.03125\n",
      "2017-10-29T22:30:25.199063: step 37, loss 8.43728, acc 0.03125\n",
      "2017-10-29T22:30:25.418218: step 38, loss 8.18463, acc 0.03125\n",
      "2017-10-29T22:30:25.665628: step 39, loss 9.65509, acc 0.03125\n",
      "2017-10-29T22:30:25.903928: step 40, loss 7.58124, acc 0.03125\n",
      "2017-10-29T22:30:26.151355: step 41, loss 9.19906, acc 0.0625\n",
      "2017-10-29T22:30:26.383121: step 42, loss 7.39571, acc 0.03125\n",
      "2017-10-29T22:30:26.621410: step 43, loss 6.99682, acc 0\n",
      "2017-10-29T22:30:26.853202: step 44, loss 8.8016, acc 0.09375\n",
      "2017-10-29T22:30:27.106746: step 45, loss 6.78572, acc 0.09375\n",
      "2017-10-29T22:30:27.338524: step 46, loss 10.1635, acc 0\n",
      "2017-10-29T22:30:27.570306: step 47, loss 6.71264, acc 0.0625\n",
      "2017-10-29T22:30:27.808604: step 48, loss 6.56732, acc 0.09375\n",
      "2017-10-29T22:30:28.040385: step 49, loss 9.37739, acc 0.03125\n",
      "2017-10-29T22:30:28.272168: step 50, loss 6.98419, acc 0.0625\n",
      "2017-10-29T22:30:28.510475: step 51, loss 8.44745, acc 0.03125\n",
      "2017-10-29T22:30:28.742257: step 52, loss 7.25683, acc 0.09375\n",
      "2017-10-29T22:30:28.974039: step 53, loss 7.25023, acc 0.0625\n",
      "2017-10-29T22:30:29.212334: step 54, loss 9.40558, acc 0\n",
      "2017-10-29T22:30:29.444114: step 55, loss 7.76185, acc 0\n",
      "2017-10-29T22:30:29.675905: step 56, loss 8.0389, acc 0.03125\n",
      "2017-10-29T22:30:29.907692: step 57, loss 9.4507, acc 0\n",
      "2017-10-29T22:30:30.130352: step 58, loss 8.23815, acc 0.03125\n",
      "2017-10-29T22:30:30.362145: step 59, loss 7.25421, acc 0.03125\n",
      "2017-10-29T22:30:30.613620: step 60, loss 8.0327, acc 0.0625\n",
      "2017-10-29T22:30:30.847404: step 61, loss 7.84617, acc 0.09375\n",
      "2017-10-29T22:30:31.063563: step 62, loss 7.04767, acc 0.0625\n",
      "2017-10-29T22:30:31.317115: step 63, loss 7.54902, acc 0.125\n",
      "2017-10-29T22:30:31.533271: step 64, loss 7.23429, acc 0.0625\n",
      "2017-10-29T22:30:31.765073: step 65, loss 7.07023, acc 0.125\n",
      "2017-10-29T22:30:31.996861: step 66, loss 7.39696, acc 0.0625\n",
      "2017-10-29T22:30:32.219517: step 67, loss 9.19227, acc 0\n",
      "2017-10-29T22:30:32.451296: step 68, loss 7.85087, acc 0.0625\n",
      "2017-10-29T22:30:32.683078: step 69, loss 7.62015, acc 0.09375\n",
      "2017-10-29T22:30:32.921372: step 70, loss 6.5983, acc 0.09375\n",
      "2017-10-29T22:30:33.153152: step 71, loss 7.71713, acc 0.03125\n",
      "2017-10-29T22:30:33.418728: step 72, loss 7.40315, acc 0.03125\n",
      "2017-10-29T22:30:33.638882: step 73, loss 9.2605, acc 0\n",
      "2017-10-29T22:30:33.870679: step 74, loss 6.91346, acc 0.0625\n",
      "2017-10-29T22:30:34.102295: step 75, loss 6.58883, acc 0.125\n",
      "2017-10-29T22:30:34.340420: step 76, loss 8.08927, acc 0.09375\n",
      "2017-10-29T22:30:34.572220: step 77, loss 7.06375, acc 0.0625\n",
      "2017-10-29T22:30:34.804028: step 78, loss 7.07754, acc 0\n",
      "2017-10-29T22:30:35.026690: step 79, loss 7.89161, acc 0.03125\n",
      "2017-10-29T22:30:35.258469: step 80, loss 7.09678, acc 0.09375\n",
      "2017-10-29T22:30:35.490254: step 81, loss 7.32462, acc 0\n",
      "2017-10-29T22:30:35.728546: step 82, loss 7.1536, acc 0\n",
      "2017-10-29T22:30:35.960333: step 83, loss 7.32208, acc 0.0625\n",
      "2017-10-29T22:30:36.192124: step 84, loss 6.67382, acc 0.0625\n",
      "2017-10-29T22:30:36.430439: step 85, loss 7.11224, acc 0.03125\n",
      "2017-10-29T22:30:36.662227: step 86, loss 7.03841, acc 0.09375\n",
      "2017-10-29T22:30:36.894014: step 87, loss 7.02047, acc 0.125\n",
      "2017-10-29T22:30:37.132318: step 88, loss 7.63141, acc 0\n",
      "2017-10-29T22:30:37.379735: step 89, loss 7.88011, acc 0.03125\n",
      "2017-10-29T22:30:37.611517: step 90, loss 6.73229, acc 0.03125\n",
      "2017-10-29T22:30:37.834181: step 91, loss 7.75448, acc 0.03125\n",
      "2017-10-29T22:30:38.081606: step 92, loss 7.54513, acc 0.0625\n",
      "2017-10-29T22:30:38.313388: step 93, loss 7.74256, acc 0\n",
      "2017-10-29T22:30:38.551682: step 94, loss 8.22734, acc 0.0625\n",
      "2017-10-29T22:30:38.783206: step 95, loss 6.26728, acc 0.09375\n",
      "2017-10-29T22:30:39.034631: step 96, loss 6.38431, acc 0.09375\n",
      "2017-10-29T22:30:39.268922: step 97, loss 7.32627, acc 0.03125\n",
      "2017-10-29T22:30:39.500707: step 98, loss 7.53208, acc 0.03125\n",
      "2017-10-29T22:30:39.735514: step 99, loss 7.25496, acc 0.0625\n",
      "2017-10-29T22:30:39.955169: step 100, loss 7.91857, acc 0.03125\n",
      "2017-10-29T22:30:40.202601: step 101, loss 7.2829, acc 0.0625\n",
      "2017-10-29T22:30:40.436394: step 102, loss 6.55981, acc 0.0625\n",
      "2017-10-29T22:30:40.672676: step 103, loss 8.0086, acc 0\n",
      "2017-10-29T22:30:40.920088: step 104, loss 6.13825, acc 0.0625\n",
      "2017-10-29T22:30:41.174005: step 105, loss 6.64674, acc 0.0625\n",
      "2017-10-29T22:30:41.405797: step 106, loss 8.05908, acc 0\n",
      "2017-10-29T22:30:41.659720: step 107, loss 6.92558, acc 0.0625\n",
      "2017-10-29T22:30:41.907131: step 108, loss 5.23492, acc 0.15625\n",
      "2017-10-29T22:30:42.161063: step 109, loss 6.52838, acc 0.09375\n",
      "2017-10-29T22:30:42.408475: step 110, loss 6.9608, acc 0\n",
      "2017-10-29T22:30:42.662396: step 111, loss 6.82371, acc 0.09375\n",
      "2017-10-29T22:30:42.893883: step 112, loss 7.36429, acc 0.0625\n",
      "2017-10-29T22:30:43.147803: step 113, loss 7.79206, acc 0.0625\n",
      "2017-10-29T22:30:43.379585: step 114, loss 6.55894, acc 0.03125\n",
      "2017-10-29T22:30:43.620147: step 115, loss 7.40805, acc 0.0625\n",
      "2017-10-29T22:30:43.851940: step 116, loss 5.8091, acc 0.09375\n",
      "2017-10-29T22:30:44.087742: step 117, loss 7.23461, acc 0.0625\n",
      "2017-10-29T22:30:44.337652: step 118, loss 6.10993, acc 0\n",
      "2017-10-29T22:30:44.569435: step 119, loss 7.73191, acc 0.09375\n",
      "2017-10-29T22:30:44.791745: step 120, loss 7.18252, acc 0.0625\n",
      "2017-10-29T22:30:45.039173: step 121, loss 7.66917, acc 0.03125\n",
      "2017-10-29T22:30:45.288597: step 122, loss 6.30352, acc 0.09375\n",
      "2017-10-29T22:30:45.509257: step 123, loss 6.40947, acc 0\n",
      "2017-10-29T22:30:45.756675: step 124, loss 6.69786, acc 0.09375\n",
      "2017-10-29T22:30:45.994981: step 125, loss 7.27241, acc 0.03125\n",
      "2017-10-29T22:30:46.226768: step 126, loss 6.91358, acc 0\n",
      "2017-10-29T22:30:46.458556: step 127, loss 6.71245, acc 0.03125\n",
      "2017-10-29T22:30:46.696851: step 128, loss 7.56794, acc 0.09375\n",
      "2017-10-29T22:30:46.928636: step 129, loss 8.16904, acc 0\n",
      "2017-10-29T22:30:47.160421: step 130, loss 7.23752, acc 0.09375\n",
      "2017-10-29T22:30:47.398722: step 131, loss 7.04033, acc 0.03125\n",
      "2017-10-29T22:30:47.630504: step 132, loss 6.02494, acc 0.0625\n",
      "2017-10-29T22:30:47.862288: step 133, loss 6.46434, acc 0.09375\n",
      "2017-10-29T22:30:48.100117: step 134, loss 6.52153, acc 0.125\n",
      "2017-10-29T22:30:48.331975: step 135, loss 6.29887, acc 0.03125\n",
      "2017-10-29T22:30:48.563765: step 136, loss 6.29453, acc 0.03125\n",
      "2017-10-29T22:30:48.802059: step 137, loss 5.67314, acc 0.09375\n",
      "2017-10-29T22:30:49.018222: step 138, loss 6.49498, acc 0.0625\n",
      "2017-10-29T22:30:49.265639: step 139, loss 7.13228, acc 0\n",
      "2017-10-29T22:30:49.503177: step 140, loss 6.09348, acc 0.0625\n",
      "2017-10-29T22:30:49.719833: step 141, loss 6.98429, acc 0\n",
      "2017-10-29T22:30:49.967261: step 142, loss 6.43889, acc 0.125\n",
      "2017-10-29T22:30:50.205565: step 143, loss 5.81555, acc 0.125\n",
      "2017-10-29T22:30:50.437347: step 144, loss 6.74275, acc 0.0625\n",
      "2017-10-29T22:30:50.684783: step 145, loss 6.73557, acc 0.125\n",
      "2017-10-29T22:30:50.923074: step 146, loss 5.8651, acc 0.03125\n",
      "2017-10-29T22:30:51.154878: step 147, loss 6.43238, acc 0.0625\n",
      "2017-10-29T22:30:51.408806: step 148, loss 6.24332, acc 0.0625\n",
      "2017-10-29T22:30:51.640592: step 149, loss 6.26877, acc 0.0625\n",
      "2017-10-29T22:30:51.888003: step 150, loss 5.96595, acc 0\n",
      "2017-10-29T22:30:52.126298: step 151, loss 6.44652, acc 0.125\n",
      "2017-10-29T22:30:52.358103: step 152, loss 6.00444, acc 0.03125\n",
      "2017-10-29T22:30:52.611990: step 153, loss 7.26091, acc 0\n",
      "2017-10-29T22:30:52.859399: step 154, loss 7.37843, acc 0.03125\n",
      "2017-10-29T22:30:53.112977: step 155, loss 7.64945, acc 0.03125\n",
      "2017-10-29T22:30:53.376030: step 156, loss 7.80658, acc 0.03125\n",
      "2017-10-29T22:30:53.614327: step 157, loss 7.03717, acc 0.03125\n",
      "2017-10-29T22:30:53.861736: step 158, loss 6.09817, acc 0.0625\n",
      "2017-10-29T22:30:54.093517: step 159, loss 6.74278, acc 0\n",
      "2017-10-29T22:30:54.347440: step 160, loss 8.03889, acc 0\n",
      "2017-10-29T22:30:54.594855: step 161, loss 7.35042, acc 0.0625\n",
      "2017-10-29T22:30:54.833150: step 162, loss 5.83415, acc 0.03125\n",
      "2017-10-29T22:30:55.064933: step 163, loss 5.64406, acc 0.125\n",
      "2017-10-29T22:30:55.314893: step 164, loss 5.54829, acc 0.125\n",
      "2017-10-29T22:30:55.565653: step 165, loss 6.9649, acc 0.03125\n",
      "2017-10-29T22:30:55.797116: step 166, loss 7.08804, acc 0.0625\n",
      "2017-10-29T22:30:56.019278: step 167, loss 6.49474, acc 0.09375\n",
      "2017-10-29T22:30:56.282493: step 168, loss 5.96784, acc 0.09375\n",
      "2017-10-29T22:30:56.498658: step 169, loss 8.18239, acc 0.0625\n",
      "2017-10-29T22:30:56.721002: step 170, loss 5.93455, acc 0.03125\n",
      "2017-10-29T22:30:56.968433: step 171, loss 6.86151, acc 0\n",
      "2017-10-29T22:30:57.199812: step 172, loss 7.26914, acc 0.03125\n",
      "2017-10-29T22:30:57.438107: step 173, loss 7.2858, acc 0.03125\n",
      "2017-10-29T22:30:57.685517: step 174, loss 7.53996, acc 0.03125\n",
      "2017-10-29T22:30:57.939446: step 175, loss 6.31071, acc 0.0625\n",
      "2017-10-29T22:30:58.186873: step 176, loss 6.24207, acc 0.0625\n",
      "2017-10-29T22:30:58.425176: step 177, loss 6.83508, acc 0.0625\n",
      "2017-10-29T22:30:58.672541: step 178, loss 7.16473, acc 0\n",
      "2017-10-29T22:30:58.925563: step 179, loss 7.25255, acc 0.09375\n",
      "2017-10-29T22:30:59.173132: step 180, loss 6.74806, acc 0.09375\n",
      "2017-10-29T22:30:59.404747: step 181, loss 6.14533, acc 0.0625\n",
      "2017-10-29T22:30:59.643041: step 182, loss 6.46907, acc 0\n",
      "2017-10-29T22:30:59.874838: step 183, loss 6.75295, acc 0.0625\n",
      "2017-10-29T22:31:00.106355: step 184, loss 7.08107, acc 0\n",
      "2017-10-29T22:31:00.329015: step 185, loss 6.27879, acc 0.03125\n",
      "2017-10-29T22:31:00.576131: step 186, loss 6.51871, acc 0.0625\n",
      "2017-10-29T22:31:00.807931: step 187, loss 6.42468, acc 0.0625\n",
      "2017-10-29T22:31:01.045782: step 188, loss 6.04582, acc 0\n",
      "2017-10-29T22:31:01.293211: step 189, loss 6.59629, acc 0.0625\n",
      "2017-10-29T22:31:01.509374: step 190, loss 6.74257, acc 0.03125\n",
      "2017-10-29T22:31:01.747460: step 191, loss 6.58689, acc 0.0625\n",
      "2017-10-29T22:31:01.994882: step 192, loss 5.5549, acc 0.1875\n",
      "2017-10-29T22:31:02.232682: step 193, loss 5.91618, acc 0.0625\n",
      "2017-10-29T22:31:02.464970: step 194, loss 6.36652, acc 0.0625\n",
      "2017-10-29T22:31:02.712383: step 195, loss 7.17646, acc 0\n",
      "2017-10-29T22:31:02.950681: step 196, loss 7.22658, acc 0\n",
      "2017-10-29T22:31:03.181830: step 197, loss 5.98771, acc 0.0625\n",
      "2017-10-29T22:31:03.435750: step 198, loss 7.43204, acc 0\n",
      "2017-10-29T22:31:03.667534: step 199, loss 6.15976, acc 0\n",
      "2017-10-29T22:31:03.899321: step 200, loss 6.40549, acc 0\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509287411\\checkpoints\\model-200\n",
      "\n",
      "2017-10-29T22:31:05.387641: step 201, loss 5.69855, acc 0.125\n",
      "2017-10-29T22:31:05.619429: step 202, loss 7.09525, acc 0.03125\n",
      "2017-10-29T22:31:05.857716: step 203, loss 5.76463, acc 0.125\n",
      "2017-10-29T22:31:06.105131: step 204, loss 6.07329, acc 0.0625\n",
      "2017-10-29T22:31:06.343430: step 205, loss 6.79931, acc 0.03125\n",
      "2017-10-29T22:31:06.559586: step 206, loss 6.69843, acc 0.03125\n",
      "2017-10-29T22:31:06.822624: step 207, loss 6.03746, acc 0.0625\n",
      "2017-10-29T22:31:07.045293: step 208, loss 5.63968, acc 0.09375\n",
      "2017-10-29T22:31:07.277077: step 209, loss 5.04763, acc 0.09375\n",
      "2017-10-29T22:31:07.524497: step 210, loss 6.82499, acc 0.0625\n",
      "2017-10-29T22:31:07.747168: step 211, loss 6.36061, acc 0.0625\n",
      "2017-10-29T22:31:07.978951: step 212, loss 6.00104, acc 0.125\n",
      "2017-10-29T22:31:08.226357: step 213, loss 6.05262, acc 0.0625\n",
      "2017-10-29T22:31:08.464650: step 214, loss 6.8269, acc 0\n",
      "2017-10-29T22:31:08.680810: step 215, loss 6.13316, acc 0\n",
      "2017-10-29T22:31:08.928217: step 216, loss 6.26648, acc 0\n",
      "2017-10-29T22:31:09.166271: step 217, loss 6.42579, acc 0.03125\n",
      "2017-10-29T22:31:09.398079: step 218, loss 6.47633, acc 0\n",
      "2017-10-29T22:31:09.652008: step 219, loss 5.63155, acc 0.03125\n",
      "2017-10-29T22:31:09.883794: step 220, loss 6.57447, acc 0.03125\n",
      "2017-10-29T22:31:10.131202: step 221, loss 5.50273, acc 0.09375\n",
      "2017-10-29T22:31:10.369499: step 222, loss 6.96726, acc 0\n",
      "2017-10-29T22:31:10.632585: step 223, loss 6.08017, acc 0.03125\n",
      "2017-10-29T22:31:10.855255: step 224, loss 6.29785, acc 0.03125\n",
      "2017-10-29T22:31:11.071014: step 225, loss 5.502, acc 0.0625\n",
      "2017-10-29T22:31:11.318426: step 226, loss 5.42713, acc 0.09375\n",
      "2017-10-29T22:31:11.572351: step 227, loss 6.01829, acc 0\n",
      "2017-10-29T22:31:11.804150: step 228, loss 6.84539, acc 0.0625\n",
      "2017-10-29T22:31:12.055775: step 229, loss 6.58391, acc 0\n",
      "2017-10-29T22:31:12.273936: step 230, loss 6.12185, acc 0.03125\n",
      "2017-10-29T22:31:12.521349: step 231, loss 6.32903, acc 0\n",
      "2017-10-29T22:31:12.759687: step 232, loss 6.84879, acc 0\n",
      "2017-10-29T22:31:12.991470: step 233, loss 6.52292, acc 0.0625\n",
      "2017-10-29T22:31:13.223255: step 234, loss 6.73935, acc 0.0625\n",
      "2017-10-29T22:31:13.461556: step 235, loss 6.4636, acc 0.03125\n",
      "2017-10-29T22:31:13.693341: step 236, loss 5.80039, acc 0.03125\n",
      "2017-10-29T22:31:13.925133: step 237, loss 6.6704, acc 0.03125\n",
      "2017-10-29T22:31:14.163076: step 238, loss 6.52284, acc 0.03125\n",
      "2017-10-29T22:31:14.379234: step 239, loss 6.47551, acc 0\n",
      "2017-10-29T22:31:14.611035: step 240, loss 6.41408, acc 0\n",
      "2017-10-29T22:31:14.842822: step 241, loss 5.47994, acc 0.03125\n",
      "2017-10-29T22:31:15.080702: step 242, loss 5.19998, acc 0.03125\n",
      "2017-10-29T22:31:15.312506: step 243, loss 6.3892, acc 0.0625\n",
      "2017-10-29T22:31:15.564427: step 244, loss 5.81024, acc 0.0625\n",
      "2017-10-29T22:31:15.782585: step 245, loss 5.73694, acc 0\n",
      "2017-10-29T22:31:16.014373: step 246, loss 5.77749, acc 0.0625\n",
      "2017-10-29T22:31:16.266360: step 247, loss 5.64767, acc 0.0625\n",
      "2017-10-29T22:31:16.499424: step 248, loss 5.81875, acc 0.09375\n",
      "2017-10-29T22:31:16.731203: step 249, loss 6.16693, acc 0.0625\n",
      "2017-10-29T22:31:16.969498: step 250, loss 6.43305, acc 0\n",
      "2017-10-29T22:31:17.201280: step 251, loss 6.03999, acc 0.0625\n",
      "2017-10-29T22:31:17.448692: step 252, loss 6.00227, acc 0.09375\n",
      "2017-10-29T22:31:17.671362: step 253, loss 6.15025, acc 0.0625\n",
      "2017-10-29T22:31:17.918361: step 254, loss 6.60427, acc 0.03125\n",
      "2017-10-29T22:31:18.149872: step 255, loss 5.8029, acc 0.0625\n",
      "2017-10-29T22:31:18.388172: step 256, loss 5.61059, acc 0.09375\n",
      "2017-10-29T22:31:18.619973: step 257, loss 6.18244, acc 0\n",
      "2017-10-29T22:31:18.869888: step 258, loss 6.25483, acc 0\n",
      "2017-10-29T22:31:19.105681: step 259, loss 6.37318, acc 0.0625\n",
      "2017-10-29T22:31:19.337465: step 260, loss 5.43963, acc 0.0625\n",
      "2017-10-29T22:31:19.575426: step 261, loss 5.17574, acc 0.09375\n",
      "2017-10-29T22:31:19.807213: step 262, loss 5.7727, acc 0.09375\n",
      "2017-10-29T22:31:20.039000: step 263, loss 5.5331, acc 0.125\n",
      "2017-10-29T22:31:20.277298: step 264, loss 7.27343, acc 0\n",
      "2017-10-29T22:31:20.509081: step 265, loss 5.96387, acc 0\n",
      "2017-10-29T22:31:20.725241: step 266, loss 6.34144, acc 0.125\n",
      "2017-10-29T22:31:20.979161: step 267, loss 6.36604, acc 0.125\n",
      "2017-10-29T22:31:21.210949: step 268, loss 6.25912, acc 0\n",
      "2017-10-29T22:31:21.442738: step 269, loss 5.74541, acc 0.03125\n",
      "2017-10-29T22:31:21.681042: step 270, loss 5.73795, acc 0.0625\n",
      "2017-10-29T22:31:21.912827: step 271, loss 5.36712, acc 0\n",
      "2017-10-29T22:31:22.159838: step 272, loss 4.86892, acc 0.1875\n",
      "2017-10-29T22:31:22.397864: step 273, loss 5.96803, acc 0.0625\n",
      "2017-10-29T22:31:22.629668: step 274, loss 4.80264, acc 0.15625\n",
      "2017-10-29T22:31:22.880587: step 275, loss 6.0468, acc 0.0625\n",
      "2017-10-29T22:31:23.115138: step 276, loss 5.85303, acc 0.09375\n",
      "2017-10-29T22:31:23.346939: step 277, loss 4.73169, acc 0.15625\n",
      "2017-10-29T22:31:23.585232: step 278, loss 6.11912, acc 0\n",
      "2017-10-29T22:31:23.817015: step 279, loss 5.35599, acc 0.03125\n",
      "2017-10-29T22:31:24.064419: step 280, loss 6.89006, acc 0.03125\n",
      "2017-10-29T22:31:24.302717: step 281, loss 5.75134, acc 0.03125\n",
      "2017-10-29T22:31:24.565753: step 282, loss 5.67092, acc 0.0625\n",
      "2017-10-29T22:31:24.803671: step 283, loss 6.351, acc 0.0625\n",
      "2017-10-29T22:31:25.035298: step 284, loss 5.6543, acc 0.03125\n",
      "2017-10-29T22:31:25.289269: step 285, loss 6.23537, acc 0\n",
      "2017-10-29T22:31:25.536367: step 286, loss 6.53247, acc 0.03125\n",
      "2017-10-29T22:31:25.790304: step 287, loss 6.09981, acc 0.125\n",
      "2017-10-29T22:31:26.037714: step 288, loss 6.30682, acc 0.0625\n",
      "2017-10-29T22:31:26.269506: step 289, loss 5.9233, acc 0.03125\n",
      "2017-10-29T22:31:26.523423: step 290, loss 6.24806, acc 0\n",
      "2017-10-29T22:31:26.770834: step 291, loss 6.08855, acc 0.03125\n",
      "2017-10-29T22:31:27.024760: step 292, loss 6.14178, acc 0.03125\n",
      "2017-10-29T22:31:27.256558: step 293, loss 5.58007, acc 0.125\n",
      "2017-10-29T22:31:27.510480: step 294, loss 5.64343, acc 0.03125\n",
      "2017-10-29T22:31:27.742276: step 295, loss 6.0043, acc 0\n",
      "2017-10-29T22:31:28.011825: step 296, loss 6.98897, acc 0.0625\n",
      "2017-10-29T22:31:28.259249: step 297, loss 6.44774, acc 0.03125\n",
      "2017-10-29T22:31:28.494043: step 298, loss 6.54428, acc 0.03125\n",
      "2017-10-29T22:31:28.729329: step 299, loss 5.74784, acc 0.09375\n",
      "2017-10-29T22:31:28.961127: step 300, loss 6.45396, acc 0\n",
      "2017-10-29T22:31:29.199429: step 301, loss 6.97337, acc 0.0625\n",
      "2017-10-29T22:31:29.431210: step 302, loss 5.39548, acc 0.03125\n",
      "2017-10-29T22:31:29.678338: step 303, loss 6.64565, acc 0\n",
      "2017-10-29T22:31:29.916636: step 304, loss 6.21738, acc 0.03125\n",
      "2017-10-29T22:31:30.148428: step 305, loss 5.89527, acc 0.03125\n",
      "2017-10-29T22:31:30.380223: step 306, loss 6.47489, acc 0.03125\n",
      "2017-10-29T22:31:30.634144: step 307, loss 6.78354, acc 0.03125\n",
      "2017-10-29T22:31:30.865950: step 308, loss 6.37194, acc 0\n",
      "2017-10-29T22:31:31.119882: step 309, loss 6.24624, acc 0.09375\n",
      "2017-10-29T22:31:31.351678: step 310, loss 5.9227, acc 0.0625\n",
      "2017-10-29T22:31:31.621238: step 311, loss 5.23083, acc 0.15625\n",
      "2017-10-29T22:31:31.837388: step 312, loss 6.98148, acc 0.03125\n",
      "2017-10-29T22:31:32.103470: step 313, loss 7.14658, acc 0.03125\n",
      "2017-10-29T22:31:32.338757: step 314, loss 5.88316, acc 0.03125\n",
      "2017-10-29T22:31:32.586163: step 315, loss 6.38677, acc 0.03125\n",
      "2017-10-29T22:31:32.840085: step 316, loss 5.81993, acc 0.125\n",
      "2017-10-29T22:31:33.071876: step 317, loss 6.71356, acc 0.03125\n",
      "2017-10-29T22:31:33.310175: step 318, loss 5.8369, acc 0.03125\n",
      "2017-10-29T22:31:33.541960: step 319, loss 5.74431, acc 0\n",
      "2017-10-29T22:31:33.773742: step 320, loss 6.71072, acc 0.03125\n",
      "2017-10-29T22:31:34.027671: step 321, loss 5.23139, acc 0.15625\n",
      "2017-10-29T22:31:34.259469: step 322, loss 5.63064, acc 0.09375\n",
      "2017-10-29T22:31:34.491255: step 323, loss 4.64869, acc 0.1875\n",
      "2017-10-29T22:31:34.745179: step 324, loss 5.38322, acc 0.09375\n",
      "2017-10-29T22:31:34.976966: step 325, loss 6.16072, acc 0.03125\n",
      "2017-10-29T22:31:35.215263: step 326, loss 5.26107, acc 0\n",
      "2017-10-29T22:31:35.462694: step 327, loss 5.55435, acc 0.09375\n",
      "2017-10-29T22:31:35.716121: step 328, loss 5.95503, acc 0.03125\n",
      "2017-10-29T22:31:35.963981: step 329, loss 5.82762, acc 0.03125\n",
      "2017-10-29T22:31:36.214902: step 330, loss 5.35526, acc 0.03125\n",
      "2017-10-29T22:31:36.449692: step 331, loss 6.282, acc 0.0625\n",
      "2017-10-29T22:31:36.696867: step 332, loss 6.2926, acc 0.0625\n",
      "2017-10-29T22:31:36.934890: step 333, loss 5.10624, acc 0.0625\n",
      "2017-10-29T22:31:37.166689: step 334, loss 5.86744, acc 0.125\n",
      "2017-10-29T22:31:37.435944: step 335, loss 4.94142, acc 0.09375\n",
      "2017-10-29T22:31:37.667331: step 336, loss 5.57236, acc 0.03125\n",
      "2017-10-29T22:31:37.917987: step 337, loss 6.48691, acc 0.03125\n",
      "2017-10-29T22:31:38.152464: step 338, loss 6.54597, acc 0.03125\n",
      "2017-10-29T22:31:38.437680: step 339, loss 6.40549, acc 0.03125\n",
      "2017-10-29T22:31:38.669461: step 340, loss 5.39702, acc 0.09375\n",
      "2017-10-29T22:31:38.920963: step 341, loss 6.2736, acc 0.125\n",
      "2017-10-29T22:31:39.139555: step 342, loss 6.71893, acc 0\n",
      "2017-10-29T22:31:39.402242: step 343, loss 5.57516, acc 0.03125\n",
      "2017-10-29T22:31:39.624909: step 344, loss 5.7135, acc 0.0625\n",
      "2017-10-29T22:31:39.872080: step 345, loss 6.35318, acc 0\n",
      "2017-10-29T22:31:40.103857: step 346, loss 6.03871, acc 0.0625\n",
      "2017-10-29T22:31:40.347617: step 347, loss 5.69804, acc 0.03125\n",
      "2017-10-29T22:31:40.584555: step 348, loss 5.72257, acc 0.0625\n",
      "2017-10-29T22:31:40.806800: step 349, loss 5.79697, acc 0.0625\n",
      "2017-10-29T22:31:41.054207: step 350, loss 6.26528, acc 0\n",
      "2017-10-29T22:31:41.301617: step 351, loss 4.72546, acc 0.125\n",
      "2017-10-29T22:31:41.523918: step 352, loss 5.76529, acc 0\n",
      "2017-10-29T22:31:41.755715: step 353, loss 6.0989, acc 0\n",
      "2017-10-29T22:31:42.009037: step 354, loss 5.65868, acc 0.166667\n",
      "2017-10-29T22:31:44.645014: step 354, loss 3.57122, acc 0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def dev_step(x_batch, y_batch, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        print('Evaluation....')\n",
    "        loops = int(len(x_batch)/32)\n",
    "        remains = len(x_batch) - 32*loops\n",
    "        count = 0\n",
    "        for i in range(loops):\n",
    "            start = i * 32\n",
    "            end = (i+1) * 32\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        for i in range(remains):\n",
    "            start = 32 * loops + i\n",
    "            end = 32 * loops + i + 1\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, float(correct_num)/len(y_batch)))\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "    # Generate batches\n",
    "    batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), 32, 1)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array([item.toarray().reshape(-1) for item in y_batch])\n",
    "        \n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 200 == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            \n",
    "    #Testing \n",
    "    dev_step(x_test, y_test, writer=dev_summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
