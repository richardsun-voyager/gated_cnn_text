{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment aims to classifiy 20newsgroup through convolutional neural network models. Based on the original [paper](http://arxiv.org/abs/1408.5882), [dennybritz](https://github.com/dennybritz/cnn-text-classification-tf) realized this model on sentimental classification using tensorflow. And he was generous to share his work at [Github](https://github.com/dennybritz/cnn-text-classification-tf). Refering to his work,  I made some modifications on the model and classify 20newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Texts\n",
    "I have preprocess the original texts and saved them as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "test_data = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in train_data.text])\n",
    "#Cut long articles to 800 words. Pad short ones\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(800)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array(list(vocab_processor.transform(test_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_data.target, test_data.target\n",
    "y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "y_test = np.array(y_test).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encode the label as one-hot code\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y_train = ohe.fit_transform(y_train)\n",
    "y_test = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Restore the values from sparse matrix\n",
    "y_train = np.array([item.toarray().reshape(-1) for item in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = np.array([item.toarray().reshape(-1) for item in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=64,\n",
    "            filter_sizes= [3, 4, 5],\n",
    "            num_filters=32,\n",
    "            l2_reg_lambda=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #train_op = optimizer.minimize(cnn.loss)\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "    \n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    for g, v in grads_and_vars:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-31T17:07:26.360456: step 100, loss 4.59662, acc 0.03125\n",
      "2017-10-31T17:07:49.951161: step 200, loss 3.82568, acc 0.0625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-200\n",
      "\n",
      "2017-10-31T17:08:16.404139: step 300, loss 3.49069, acc 0.125\n",
      "2017-10-31T17:08:40.058145: step 400, loss 3.64796, acc 0.09375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-400\n",
      "\n",
      "2017-10-31T17:09:06.384396: step 500, loss 2.84871, acc 0.1875\n",
      "2017-10-31T17:09:30.166064: step 600, loss 3.17099, acc 0.0625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-600\n",
      "\n",
      "2017-10-31T17:09:56.475142: step 700, loss 2.97174, acc 0.09375\n",
      "2017-10-31T17:10:19.930916: step 800, loss 2.91651, acc 0.09375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-800\n",
      "\n",
      "2017-10-31T17:10:46.476919: step 900, loss 2.73413, acc 0.15625\n",
      "2017-10-31T17:11:10.001595: step 1000, loss 2.12382, acc 0.375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-1000\n",
      "\n",
      "2017-10-31T17:11:36.496683: step 1100, loss 2.54332, acc 0.21875\n",
      "2017-10-31T17:12:00.206861: step 1200, loss 2.44007, acc 0.34375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-1200\n",
      "\n",
      "2017-10-31T17:12:26.724324: step 1300, loss 2.27162, acc 0.375\n",
      "2017-10-31T17:12:50.361459: step 1400, loss 1.80303, acc 0.53125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-1400\n",
      "\n",
      "2017-10-31T17:13:16.638662: step 1500, loss 1.80003, acc 0.5625\n",
      "2017-10-31T17:13:40.329837: step 1600, loss 1.86095, acc 0.46875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-1600\n",
      "\n",
      "2017-10-31T17:14:06.852641: step 1700, loss 1.72223, acc 0.53125\n",
      "2017-10-31T17:14:30.450706: step 1800, loss 1.88921, acc 0.59375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-1800\n",
      "\n",
      "2017-10-31T17:14:56.707808: step 1900, loss 1.71726, acc 0.5625\n",
      "2017-10-31T17:15:20.333270: step 2000, loss 1.56156, acc 0.625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-2000\n",
      "\n",
      "2017-10-31T17:15:47.310705: step 2100, loss 1.7074, acc 0.5625\n",
      "2017-10-31T17:16:10.824652: step 2200, loss 1.47224, acc 0.71875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-2200\n",
      "\n",
      "2017-10-31T17:16:37.217570: step 2300, loss 1.41668, acc 0.65625\n",
      "2017-10-31T17:17:01.397475: step 2400, loss 1.27955, acc 0.75\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-2400\n",
      "\n",
      "2017-10-31T17:17:28.626486: step 2500, loss 1.20477, acc 0.6875\n",
      "2017-10-31T17:17:52.808158: step 2600, loss 1.5917, acc 0.65625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-2600\n",
      "\n",
      "2017-10-31T17:18:19.638841: step 2700, loss 1.34793, acc 0.59375\n",
      "2017-10-31T17:18:43.720447: step 2800, loss 1.05982, acc 0.75\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-2800\n",
      "\n",
      "2017-10-31T17:19:10.754533: step 2900, loss 0.827964, acc 0.84375\n",
      "2017-10-31T17:19:35.095305: step 3000, loss 0.827084, acc 0.78125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-3000\n",
      "\n",
      "2017-10-31T17:20:02.619665: step 3100, loss 1.01455, acc 0.6875\n",
      "2017-10-31T17:20:26.492200: step 3200, loss 0.708313, acc 0.875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-3200\n",
      "\n",
      "2017-10-31T17:20:53.524612: step 3300, loss 0.824181, acc 0.8125\n",
      "2017-10-31T17:21:17.439165: step 3400, loss 1.02074, acc 0.8125\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-3400\n",
      "\n",
      "2017-10-31T17:21:44.308828: step 3500, loss 0.894145, acc 0.75\n",
      "2017-10-31T17:22:07.868867: step 3600, loss 0.618745, acc 0.875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-3600\n",
      "\n",
      "2017-10-31T17:22:34.892702: step 3700, loss 0.666221, acc 0.9375\n",
      "2017-10-31T17:22:58.627462: step 3800, loss 0.722557, acc 0.84375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-3800\n",
      "\n",
      "2017-10-31T17:23:25.251444: step 3900, loss 0.512936, acc 0.96875\n",
      "2017-10-31T17:23:48.842546: step 4000, loss 0.569852, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-4000\n",
      "\n",
      "2017-10-31T17:24:15.371430: step 4100, loss 1.17439, acc 0.8125\n",
      "2017-10-31T17:24:39.996520: step 4200, loss 0.527003, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-4200\n",
      "\n",
      "2017-10-31T17:25:06.578464: step 4300, loss 0.457983, acc 0.90625\n",
      "2017-10-31T17:25:30.473768: step 4400, loss 0.567174, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-4400\n",
      "\n",
      "2017-10-31T17:25:57.078333: step 4500, loss 0.58319, acc 0.90625\n",
      "2017-10-31T17:26:20.723885: step 4600, loss 0.611645, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-4600\n",
      "\n",
      "2017-10-31T17:26:47.161354: step 4700, loss 0.468819, acc 0.9375\n",
      "2017-10-31T17:27:10.746263: step 4800, loss 0.55024, acc 0.875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-4800\n",
      "\n",
      "2017-10-31T17:27:37.196029: step 4900, loss 0.380391, acc 0.96875\n",
      "2017-10-31T17:28:00.770966: step 5000, loss 0.456976, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-5000\n",
      "\n",
      "2017-10-31T17:28:27.290010: step 5100, loss 0.523694, acc 0.90625\n",
      "2017-10-31T17:28:50.949667: step 5200, loss 0.512008, acc 0.90625\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-5200\n",
      "\n",
      "2017-10-31T17:29:17.364466: step 5300, loss 0.615105, acc 0.90625\n",
      "2017-10-31T17:29:40.937904: step 5400, loss 0.444394, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-5400\n",
      "\n",
      "2017-10-31T17:30:07.368307: step 5500, loss 0.561639, acc 0.90625\n",
      "2017-10-31T17:30:31.073465: step 5600, loss 0.354705, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-5600\n",
      "\n",
      "2017-10-31T17:30:57.543176: step 5700, loss 0.351091, acc 0.96875\n",
      "2017-10-31T17:31:21.287441: step 5800, loss 0.376037, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-5800\n",
      "\n",
      "2017-10-31T17:31:47.535137: step 5900, loss 0.325014, acc 0.9375\n",
      "2017-10-31T17:32:11.162867: step 6000, loss 0.315535, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-6000\n",
      "\n",
      "2017-10-31T17:32:37.593318: step 6100, loss 0.359008, acc 0.96875\n",
      "2017-10-31T17:33:01.310453: step 6200, loss 0.331099, acc 0.9375\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-6200\n",
      "\n",
      "2017-10-31T17:33:27.813723: step 6300, loss 0.371819, acc 0.90625\n",
      "2017-10-31T17:33:51.381867: step 6400, loss 0.371629, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-6400\n",
      "\n",
      "2017-10-31T17:34:17.770496: step 6500, loss 0.253807, acc 1\n",
      "2017-10-31T17:34:41.320762: step 6600, loss 0.326453, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-6600\n",
      "\n",
      "2017-10-31T17:35:07.732941: step 6700, loss 0.30433, acc 0.96875\n",
      "2017-10-31T17:35:31.347387: step 6800, loss 0.297747, acc 0.96875\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-6800\n",
      "\n",
      "2017-10-31T17:35:57.787670: step 6900, loss 0.406503, acc 0.96875\n",
      "2017-10-31T17:36:21.427412: step 7000, loss 0.233533, acc 1\n",
      "Saved model checkpoint to C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-7000\n",
      "\n",
      "Evaluation....\n",
      "2017-10-31T17:36:45.922047: step 7080, loss 0.137884, acc 0.000132767\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step%100 == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def dev_step(x_batch, y_batch, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        print('Evaluation....')\n",
    "        loops = int(len(x_batch)/32)\n",
    "        remains = len(x_batch) - 32*loops\n",
    "        count = 0\n",
    "        for i in range(loops):\n",
    "            start = i * 32\n",
    "            end = (i+1) * 32\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        for i in range(remains):\n",
    "            start = 32 * loops + i\n",
    "            end = 32 * loops + i + 1\n",
    "            x = x_batch[start: end]\n",
    "            y = y_batch[start: end]\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x,\n",
    "                cnn.input_y: y,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, correct_num = sess.run([global_step, dev_summary_op, cnn.loss, cnn.correct_num],\n",
    "            feed_dict)\n",
    "            count += correct_num\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, float(correct_num)/len(y_batch)))\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "    # Generate batches\n",
    "    batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), 32, 20)\n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "        \n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 200 == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\onlooker\\Documents\\deeplearning_projects\\text_classification_cnn\\cnn-text-classification-tf\\runs\\1509440394\\checkpoints\\model-7000\n",
      "Accuracy:0.77403\n"
     ]
    }
   ],
   "source": [
    "#Restore the parameters and do testing\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    model_file=tf.train.latest_checkpoint('runs/1509440394/checkpoints/')\n",
    "    saver.restore(sess, model_file)\n",
    "    loops = int(len(x_test)/32)\n",
    "    remains = len(x_test) - 32*loops\n",
    "    count = 0\n",
    "    for i in range(loops):\n",
    "        start = i * 32\n",
    "        end = (i+1) * 32\n",
    "        x = x_test[start: end]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x,\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "        correct_num = sess.run(cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "    for i in range(remains):\n",
    "        start = 32 * loops + i\n",
    "        end = 32 * loops + i + 1\n",
    "        x = x_test[start: ]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x,\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        correct_num = sess.run(cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"Accuracy:{:.5f}\".format(float(count)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
