{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "test_data = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in train_data.text])\n",
    "max_document_length = max_document_length if max_document_length < 800 else 800\n",
    "#Cut long articles to 800 words. Pad short ones\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data.text)))\n",
    "x_test = np.array(list(vocab_processor.transform(test_data.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_data.target, test_data.target\n",
    "#y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "#y_test = np.array(y_test).reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#ohe = OneHotEncoder()\n",
    "#y_train = ohe.fit_transform(y_train)\n",
    "#y_test = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Restore the values from sparse matrix\n",
    "#y_train = np.array([item.toarray().reshape(-1) for item in y_train])\n",
    "#y_test = np.array([item.toarray().reshape(-1) for item in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_size = 2000\n",
    "    class_num = 20\n",
    "    embedding_size = 32\n",
    "    filter_size = 8\n",
    "    num_layers = 5\n",
    "    block_size = 5\n",
    "    filter_h = 5 #conv_kernel height_size\n",
    "    doc_len = 800#context_size\n",
    "    context_size = 800#context_size after padding\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    num_sampled = 1\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.99\n",
    "    num_batches = 0\n",
    "    ckpt_path = \"cpkt\"\n",
    "    summary_path = \"logs\"\n",
    "    data_dir = \"data\"\n",
    "    grad_clip = 6\n",
    "    \n",
    "class testConfig:\n",
    "    vocab_size = 2000\n",
    "    class_num = 20\n",
    "    embedding_size = 32\n",
    "    filter_size = 8\n",
    "    num_layers = 5\n",
    "    block_size = 5\n",
    "    filter_h = 5 #conv_kernel height_size\n",
    "    doc_len = 800#context_size\n",
    "    context_size = 800#context_size after padding\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    num_sampled = 1\n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.99\n",
    "    num_batches = 0\n",
    "    ckpt_path = \"cpkt\"\n",
    "    summary_path = \"logs\"\n",
    "    data_dir = \"data\"\n",
    "    grad_clip = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_conf(conf):\n",
    "    conf.filter_w = conf.embedding_size\n",
    "    #Pad the first k-1 item\n",
    "    conf.context_size = conf.doc_len + int((conf.filter_h - 1)/2)\n",
    "    \n",
    "    # Check if data exists\n",
    "    if not os.path.exists(conf.data_dir):\n",
    "        exit(\"Please download the data as mentioned in Requirements\")\n",
    "\n",
    "    # Create paths for checkpointing\n",
    "    #ckpt_model_path = 'vocab%d_embed%d_filters%d_batch%d_layers%d_block%d_fdim%d'%(conf.vocab_size, conf.embedding_size, \n",
    "            #conf.filter_size, conf.batch_size, conf.num_layers, conf.block_size, conf.filter_h)\n",
    "    #conf.ckpt_path = os.path.join(conf.ckpt_path, ckpt_model_path)\n",
    "    conf.ckpt_path = 'cpkt'\n",
    "\n",
    "    if not os.path.exists(conf.ckpt_path):\n",
    "        os.makedirs(conf.ckpt_path)\n",
    "    conf.ckpt_file = os.path.join(conf.ckpt_path, \"model.ckpt\")\n",
    "\n",
    "    # Create Logs Folder\n",
    "    if tf.gfile.Exists(conf.summary_path):\n",
    "        tf.gfile.DeleteRecursively(conf.summary_path)\n",
    "    tf.gfile.MakeDirs(conf.summary_path)\n",
    "    return conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below code was referred to http://danijar.com/structuring-your-tensorflow-models/\n",
    "import functools\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator decorator, allowing to use the decorator to be used without\n",
    "    parentheses if not arguments are provided. All arguments must be optional.\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A decorator for functions that define TensorFlow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result so that operations are added to the graph only once.\n",
    "\n",
    "    The operations added by the function live within a tf.variable_scope(). If\n",
    "    this decorator is used with arguments, they will be forwarded to the\n",
    "    variable scope. The scope name defaults to the name of the wrapped\n",
    "    function.\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__#Get function name\n",
    "    name = scope or function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)#Keep the original function\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):#If the attribute not exist\n",
    "            with tf.variable_scope(name, *args, **kwargs):#Add scope name\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)#otherwise return the attribute\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Created a Gated CNN model\n",
    "class GatedCNN(object):\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        ##tf.reset_default_graph()\n",
    "        #Input is a series of words\n",
    "        #Paddle the first beginning k-1 values as zeros\n",
    "        #doc_len = conf.doc_len + filter_h - 1\n",
    "        self.X = tf.placeholder(shape=[conf.batch_size, conf.context_size], dtype=tf.int32, name=\"X\")\n",
    "        self.y = tf.placeholder(shape=[conf.batch_size], dtype=tf.int32, name=\"y\")\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            embed = self.create_embeddings(self.X, conf)\n",
    "            h, res_input = embed, embed\n",
    "        \n",
    "        with tf.name_scope(\"GatedConvLayers\"):\n",
    "            for i in range(conf.num_layers):\n",
    "                #Channels of current input\n",
    "                fanin_depth = h.get_shape()[-1]\n",
    "                #Set current filter size\n",
    "                filter_size = conf.filter_size if i < conf.num_layers-1 else 1\n",
    "                shape = (conf.filter_h, conf.filter_w, fanin_depth, filter_size)\n",
    "                with tf.variable_scope(\"layer_%d\"%i):\n",
    "                    conv_w = self.conv_op(h, shape, \"linear\")\n",
    "                    conv_v = self.conv_op(h, shape, \"gated\")\n",
    "                    h = conv_w * tf.sigmoid(conv_v)\n",
    "                    if i % conf.block_size == 0:\n",
    "                        h += res_input\n",
    "                        res_input = h\n",
    "            \n",
    "        #h = tf.reshape(h, (-1, conf.embedding_size))\n",
    "        h = tf.squeeze(h)\n",
    "        #Get the last one as the hidden state\n",
    "        h = h[:, -1, :]\n",
    "        #print(h)\n",
    "        #Flatten\n",
    "        #h = tf.reshape(h, [conf.batch_size, -1])\n",
    "        h_final_size = h.get_shape()[-1]\n",
    "        y_shape = self.y.get_shape().as_list()\n",
    "        \n",
    "        #Fully connected layer\n",
    "        #with tf.variable_scope('Fully_Connected_Layer'):\n",
    "            #f_w = tf.get_variable(\"fully_w\", [h_final_size, conf.embedding_size], tf.float32, \n",
    "                                    #tf.random_normal_initializer(0.0, 0.1))\n",
    "            #f_b = tf.get_variable(\"fully_b\", [conf.embedding_size], tf.float32, \n",
    "                                    #tf.constant_initializer(1.0))\n",
    "            #h_fully = tf.matmul(h, f_w) + f_b\n",
    "\n",
    "        #self.y = tf.reshape(self.y, (y_shape[0] * y_shape[1], 1))\n",
    "        #Transform y into one-hot\n",
    "        #y = tf.one_hot(self.y, conf.class_num)\n",
    "        with tf.variable_scope('Output'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [conf.embedding_size, conf.class_num], tf.float32, \n",
    "                                    tf.random_normal_initializer(0.0, 0.1))\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [conf.class_num], tf.float32, \n",
    "                                    tf.constant_initializer(1.0))\n",
    "        \n",
    "            logits = tf.matmul(h, softmax_w) + softmax_b\n",
    "        pred = tf.nn.softmax(logits)\n",
    "        prediction = tf.argmax(pred, 1)\n",
    "        prediction = tf.cast(prediction, tf.int32)\n",
    "        correct_prediction = tf.equal(prediction, self.y)\n",
    "        self.correct_num = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=logits)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "\n",
    "        trainer = tf.train.MomentumOptimizer(conf.learning_rate, conf.momentum)\n",
    "        gradients = trainer.compute_gradients(self.loss)\n",
    "        clipped_gradients = [(tf.clip_by_value(_[0], -conf.grad_clip, conf.grad_clip), _[1]) for _ in gradients]\n",
    "        self.optimizer = trainer.apply_gradients(clipped_gradients)\n",
    "        #self.perplexity = tf.exp(self.loss)\n",
    "\n",
    "        self.create_summaries()\n",
    "\n",
    "    def create_embeddings(self, X, conf):\n",
    "\n",
    "        embeddings = tf.get_variable(\"embeds\",(conf.vocab_size, conf.embedding_size), tf.float32, \n",
    "                                     tf.random_uniform_initializer(-1.0,1.0))\n",
    "        \n",
    "        embed = tf.nn.embedding_lookup(embeddings, X)\n",
    "        batch_size = self.X.get_shape()[0]\n",
    "        mask_layer = np.ones((conf.batch_size, conf.context_size, conf.embedding_size))\n",
    "        #Zero Pad the first beginning k-1 values of the input\n",
    "        #In order to \n",
    "        #Batch Length Embedding\n",
    "        k = int(conf.filter_h/2)\n",
    "        mask_layer[:, :k, :] = 0\n",
    "        embed *= mask_layer\n",
    "        \n",
    "        embed_shape = embed.get_shape().as_list()\n",
    "        embed = tf.reshape(embed, (embed_shape[0], embed_shape[1], embed_shape[2], 1))\n",
    "        #expand_dim\n",
    "        return embed\n",
    "\n",
    "\n",
    "    def conv_op(self, fan_in, shape, name):\n",
    "        W = tf.get_variable(\"%s_W\"%name, shape, tf.float32, tf.random_normal_initializer(0.0, 0.1))\n",
    "        b = tf.get_variable(\"%s_b\"%name, shape[-1], tf.float32, tf.constant_initializer(1.0))\n",
    "        #Note the padding method is 'SAME', it will automatically pad the first k/2 items\n",
    "        return tf.add(tf.nn.conv2d(fan_in, W, strides=[1,1,1,1], padding='SAME'), b)\n",
    "    \n",
    "    def create_summaries(self):\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        #tf.summary.scalar(\"perplexity\", self.perplexity)\n",
    "        self.merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = config  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = prepare_conf(conf)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    gated_cnn = GatedCNN(conf)\n",
    "    saver = tf.train.Saver(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0  Loss:20.924\n",
      "step  100  Loss:2.995\n",
      "step  200  Loss:3.012\n",
      "step  300  Loss:2.973\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (18,) for Tensor 'y:0', which has shape '(32,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a7de1c06b6c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[1;31m#Training starts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mgated_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_batch_pad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgated_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgated_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgated_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' Loss:{:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1101\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (18,) for Tensor 'y:0', which has shape '(32,)'"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Generate batches\n",
    "    batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), conf.batch_size, 1)\n",
    "    # Training loop. For each batch...\n",
    "    for i, batch in enumerate(batches):\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        x_batch = np.array(x_batch)\n",
    "        x_batch_pad = np.zeros([conf.batch_size, conf.context_size])\n",
    "        x_batch_pad[:, :conf.doc_len]\n",
    "        y_batch = np.array(y_batch)\n",
    "        #Training starts\n",
    "        feed_dict = {gated_cnn.X: x_batch_pad, gated_cnn.y: y_batch}\n",
    "        _, l = sess.run([gated_cnn.optimizer, gated_cnn.loss], feed_dict=feed_dict)\n",
    "        if i % 100 == 0:\n",
    "            print('step ', i, ' Loss:{:.3f}'.format(l))\n",
    "            #saver.save(sess, save_path=conf.ckpt_path)\n",
    "            \n",
    "    loops = len(y_test)/32\n",
    "    count = 0\n",
    "    for i in range(loops):\n",
    "        start = i * 32\n",
    "        end = (i+1) * 32\n",
    "        x = x_test[start: end]\n",
    "        y = y_test[start: end]\n",
    "        feed_dict = {\n",
    "            gated_cnn.X: x,\n",
    "            gated_cnn.y: y\n",
    "            }\n",
    "        correct_num = sess.run(gated_cnn.correct_num, feed_dict)\n",
    "        count += correct_num\n",
    "        \n",
    "    count/float(32*loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method save in module tensorflow.python.training.saver:\n",
      "\n",
      "save(sess, save_path, global_step=None, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True, write_state=True) method of tensorflow.python.training.saver.Saver instance\n",
      "    Saves variables.\n",
      "    \n",
      "    This method runs the ops added by the constructor for saving variables.\n",
      "    It requires a session in which the graph was launched.  The variables to\n",
      "    save must also have been initialized.\n",
      "    \n",
      "    The method returns the path of the newly created checkpoint file.  This\n",
      "    path can be passed directly to a call to `restore()`.\n",
      "    \n",
      "    Args:\n",
      "      sess: A Session to use to save the variables.\n",
      "      save_path: String.  Path to the checkpoint filename.  If the saver is\n",
      "        `sharded`, this is the prefix of the sharded checkpoint filename.\n",
      "      global_step: If provided the global step number is appended to\n",
      "        `save_path` to create the checkpoint filename. The optional argument\n",
      "        can be a `Tensor`, a `Tensor` name or an integer.\n",
      "      latest_filename: Optional name for the protocol buffer file that will\n",
      "        contains the list of most recent checkpoint filenames.  That file,\n",
      "        kept in the same directory as the checkpoint files, is automatically\n",
      "        managed by the saver to keep track of recent checkpoints.  Defaults to\n",
      "        'checkpoint'.\n",
      "      meta_graph_suffix: Suffix for `MetaGraphDef` file. Defaults to 'meta'.\n",
      "      write_meta_graph: `Boolean` indicating whether or not to write the meta\n",
      "        graph file.\n",
      "      write_state: `Boolean` indicating whether or not to write the\n",
      "        `CheckpointStateProto`.\n",
      "    \n",
      "    Returns:\n",
      "      A string: path at which the variables were saved.  If the saver is\n",
      "        sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'\n",
      "        is the number of shards created.\n",
      "      If the saver is empty, returns None.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: If `sess` is not a `Session`.\n",
      "      ValueError: If `latest_filename` contains path components, or if it\n",
      "        collides with `save_path`.\n",
      "      RuntimeError: If save and restore ops weren't built.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(saver.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
